<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Programming | The Analytic Bastard]]></title>
  <link href="http://analyticbastard.github.io/blog/categories/programming/atom.xml" rel="self"/>
  <link href="http://analyticbastard.github.io/"/>
  <updated>2015-05-21T00:31:28+02:00</updated>
  <id>http://analyticbastard.github.io/</id>
  <author>
    <name><![CDATA[Analytic Bastard]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Max Subsequence Problem]]></title>
    <link href="http://analyticbastard.github.io/blog/2015/05/02/max-subsequence-problem/"/>
    <updated>2015-05-02T21:05:23+02:00</updated>
    <id>http://analyticbastard.github.io/blog/2015/05/02/max-subsequence-problem</id>
    <content type="html"><![CDATA[<p>The following brainteaser is about finding the maximum-sum subsequence within a given sequence.
We will solve this problem in Clojure, sice this will involve less pain than doing it in Java.</p>

<p>We need to decompose the sequence in parts, and a natural way of doing it is dividing it in halves,
using a binary tree. For this purpose, we take the sequence at the current stage, divide it in two
parts, and call the same function recursively for each half. The recursion-stopping criteria are the point when
we find either two or only one element in the sequence (odd or even super sequence), at which point
we return the information we need (we’ll talk about this later on).</p>

<p>&#8220;` Clojure
(defn best [myseq]
  (let [N (count myseq)]
    (case N
      1 nil
      2 nil
      (let [n (Math/round (double (/ N 2)))
            m (- N n)
            s1 (take n myseq)
            s2 (take-last m myseq)
            m1 (best s1)
            m2 (best s2)
            ]</p>

<pre><code>    ))
)) ``` 
</code></pre>

<p>This was the basic recursion skeleton. Now we need to return and process the interesting information
about the sequence at each stage. We need the maximum-sum sub-sequence at each stage in the binary
search, the discarded sequence at the left, and the discarded sequence at the right (both of which must be
negative because they would otherwise increase the sum). For example, the sequence <code>[-1 -1 3 4 -2]</code> would
produce a best subsequence <code>[3 4]</code>, a discarded left sequence <code>[-1 -1]</code> and a discarded right sequence
[-2]. Optionally and for the sake of performance, we also return the sums of these sequences, so that
we don’t need to recompute at the upper-levels (when returning from the call). The information retrieved from
the lower levels is a map with this structure (using the previous example):</p>

<p>&#8220;` Clojure</p>
<p>&#8220;`</p>

<p>where the first three keys are the sums, and the last three keys are the proper sequences. We now build
the information according to the sequence we recieve as input. When there is only one element, it is easy to
see that we need to return empty discarded sequences, and the input number as the best sequence. If we get
two elements, we need to check whether both are positive, in which case, we return both as the best sequence
(and empty discarded left and right sequences). Otherwise, get the maximum, return it as the best sequence,
and return the other as either the left or right discarded sequence depending on whether it was the first or
second in the sequence. Note, at this point, that the discarded sequences mean how large the penalization is
to get to the best sequence for a given subsequence from the left or the right. In the recursion stage, when
we have two subsequences, we need to check if the total sum of both best subsequences and everything in the
middle (the right discarded sequence from the left half and the left discarded sequence from the right half)
is larger than the maximum of the best subsequence of either half. We concatenate the sequences to compose
the left and right discarded sequences, and the best sequences up to this stage. This is the result:</p>

<p><code>Clojure
(defn best [myseq]
  (let [N (count myseq)]
    (case N
      1 {:l 0 :r 0 :sl [] :sr [] :seq myseq :sum (first myseq)}
      2 (if (and (&gt; (first myseq) 0) (&gt; (second myseq) 0))
          {:l 0 :r 0 :sl [] :seq myseq :sr [] :sum (apply + myseq)}
          (if (&lt; (first myseq) (second myseq))
            {:l (first myseq) :r 0 :sl [(first myseq)] :sr [] :seq (rest myseq) :sum (second myseq)}
            {:l 0 :r (second myseq) :sl [] :sr (rest myseq) :seq [(first myseq)] :sum (first myseq)}))
      (let [n (Math/round (double (/ N 2)))
            m (- N n)
            s1 (take n myseq)
            s2 (reverse (take m (reverse myseq)))
            m1 (best s1)
            m2 (best s2)
            sum2 (+ (:sum m1) (:sum m2))
            cost (+ (:r m1) (:l m2))
            sumt (+ sum2 cost)
            ]
        (if (&gt; sumt (max (:sum m1) (:sum m2)))
          {:l (:l m1) :r (:r m2) :sl (:sl m1) :sr (:sr m2) :seq (concat (:seq m1) (:sr m1) (:sl m2) (:seq m2)) :sum sumt}
          (if (&gt; (:sum m1) (:sum m2))
            {:l (:l m1) :r (+ (:r m1) (:l m2) (:sum m2) (:r m2)) :sl (:sl m1) :sr (concat (:sr m1) (:sl m2) (:seq m2) (:sr m2)) :seq (:seq m1) :sum (:sum m1)}
            {:l (+ (:l m1) (:r m1) (:sum m1) (:l m2)) :r (:r m2) :sl (concat (:sl m1) (:seq m1) (:sr m1) (:sl m2)) :sr (:sr m2) :seq (:seq m2) :sum (:sum m2)}))
        ))
    ))
</code></p>

<p>As test data, we choose three sequences, one whose best subsequence is in the middle, separated by negative numbers
(one in this case), another whose best subsequence is at the left side, and it is not worth to join it with another
positive sequence at the right side, and one whose best subsequence is itself.</p>

<p><code>Clojure
(def ex-bestmiddle [-1 -3 -2 1 2 -1 3 -2 2 1 -1 -3])
(def ex-bestleft [-1 3 2 1 -2 -5 -3 -3 2 3 -1 -3])
(def ex-bestall [5 -3 -2 2 2 -1 3 -2 2 1 -1 3])
</code></p>

<p>For the last one:</p>

<p><code>Clojure
(best ex-bestall)
</code></p>

<p>&#8220;` Clojure</p>
<p>&#8220;`</p>

<p>For the best subsequence on the left side:</p>

<p><code>Clojure
(best ex-bestleft)
</code></p>

<p>&#8220;` Clojure</p>
<p>&#8220;`</p>

<p>And for the best subsequence in the middle:</p>

<p><code>Clojure
(best ex-bestmiddle)
</code></p>

<p>&#8220;` Clojure</p>
<p>&#8220;`</p>

<p>The code is on <a href="https://github.com/analyticbastard/java-exercises">Github</a> under <code>src/main/clj</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Manipulating SVG Images From Browser Javascript]]></title>
    <link href="http://analyticbastard.github.io/blog/2015/04/09/manipulating-svg-images-from-browser-javascript/"/>
    <updated>2015-04-09T01:18:48+02:00</updated>
    <id>http://analyticbastard.github.io/blog/2015/04/09/manipulating-svg-images-from-browser-javascript</id>
    <content type="html"><![CDATA[<p>SVG files are vector images based on a XML specification for describing geometric shapes (vector images). The fact that the image itself is an XML element makes it an ideal candidate to be manipulated from our client-side code. Unfortunately, SVG elements are not entirely treated in the same way that HTML elements are. In particular, they hold an internal DOM.</p>

<p>I am working on a project that involves dealing with a chart produced by the D3.js library, which produces SVG images.</p>

<p>In particular, I could convert a collection of labels whose XML tag was “text” within the SVG image to a Javascript array in this way:</p>

<p><code>Javascript
var labelArray = [].slice.call(document.getElementsByTagName("text"));
</code></p>

<p>Then I used the Functional Javascript library, which is cool, to select the element I was looking for</p>

<p><code>Javascript
var label = fjs.first(function(elem) {
                    // textContent is an attribute of SVG element type
                    return elem.textContent === "ad";
})(labelsArray);
</code></p>

<p>And then set special properties on it, so it would be distinguishable from the rest of the elements:</p>

<p><code>Javascript
label.style.fontSize = "20px";
label.style.fontWeight = "bold";
</code></p>

<p>Otherwise, elements with specific ID could be accessed firstly by getting the SVG file content from the SVG DOM element first, then accessing the internal element by ID:</p>

<p><code>Javascript
var a = document.getElementById("imagesvg");
a.addEventListener("load",function(){
            var svgDoc = a.contentDocument; //get the inner DOM of image.svg
            var delta = svgDoc.getElementById("delta"); //get the inner element by id
},false)
</code></p>

<p>Notice that the file is called image.svg and that the browser can load it asynchronously, and that’s the reason for the load event handler.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[My Git Workflow]]></title>
    <link href="http://analyticbastard.github.io/blog/2015/01/10/my-git-workflow/"/>
    <updated>2015-01-10T01:00:55+01:00</updated>
    <id>http://analyticbastard.github.io/blog/2015/01/10/my-git-workflow</id>
    <content type="html"><![CDATA[<p>In the past I had used CVS as most of the people. Git is much powerful, in so many levels. For starters, you never,
NEVER lose files. You can <strong>always</strong> go back to a previous version, the soft ware, or the hard. But Git requires a little
more attention on the part of the user than other control version systems. This starts by fixing the basic concepts
clearly.</p>

<h3 id="concepts">Concepts</h3>

<p>The concepts, from basic qto more complex, and also sorted by the first time you are likely to come across them, is:</p>

<ul>
  <li><strong>Tracked file</strong>: A file that is under version control. You specifically told Git to track it at some point of its life.</li>
  <li><strong>Woring directory</strong>: This is the classical concept, your working copy of the project, including the files you
modify, the ones that are tracked and others that you want to leave untracked (such as IDE files or personal notes).</li>
  <li><strong>Local repository</strong>: This is <em>the Git repository stored on your local computer</em>. It is <em>not</em> your local working
directory, but the files stored in the <code>.git</code> directory within your working copy.</li>
  <li><strong>remote</strong>: A remote is a remote Git server that stores our project. Several remotes can exist for a project and we
always need to specify the remote to which we are going to send our changes.</li>
  <li><strong>Remote repository</strong>: The remote repository that stores the version of the project that everybody can see.</li>
  <li><strong>Branch</strong>: This is the usual concept of branch. Branches of the project may evolve in an independent way from each
other. All commits are attached to a branch. Local branches may not exist in the remote repository and remote branches
may have not been updated in your local repository at some point. The default branch is called <em>master</em>.</li>
  <li><strong>Conflict</strong>: Incompatible changes from a source were incorporated to a dirty copy (meaning that you modify something
that had been already modified), or you have a commit that modifies something already modified in the source you are
applying. You will need to resolve this by hand.</li>
  <li><strong>stash</strong>: Shelve your local changes. In other words, pack your dirty working copy and leave a clean working directory
that matches the contents of your local repository. This is important if you want to do operations on files that you
have changed on your working copy, since Git will complain otherwise.</li>
  <li><strong>unstash</strong>: Get the changes from the stash into your local copy, making it dirty if it was not. If it was not dirty
or you got changes from another branch or from the remote repository, you are likely to have conflicts and will need
to resolve them.</li>
  <li><strong>pull</strong>: Update remote changes in your local repository (and your working copy so you can see them). If you have
a dirty working copy you will need to stash your local changes.</li>
  <li><strong>Dirty local copy</strong>: Tracked files in the working directory that have been changed.</li>
  <li><strong>commit</strong>: A change in a tracked file (dirty local copy) that gets added to the local repository. You commit to a
branch.</li>
  <li><strong>push</strong>: The commits (local changes that now form part of your local repository) get sent to the remote repository
so that everybody can finally access them. This updates the remote branch.</li>
  <li><strong>merge</strong>: This happens when two members have changed the same tracked files on a branch in the the remote
repository. The last user in pushing the changes (<em>B</em>) will need to pull the changes that the first one (<em>A</em>) already pushed,
so that the official history was written firstly by <em>A</em>. The conflicts will most likely need to be resolved by
hand by <em>B</em>, since Git does not understand any particular language (and is not intelligent to decide on the conflicts
or have criteria to do so). Once merged, <em>B</em> ends up with a dirty working copy, that he needs to commit and then push.
This last push will be accepted by the remote repository server (if no one has put new stuff in the meantime).</li>
  <li><strong>rebase</strong>: This is a especially useful type of merge in which a user has not pulled down changes for a while and there
are a number of commits in the remote repository. If the user commits, then he can rebase the remote branch onto his,
so that his commit is merged on top of the remote commits. The user can then push to the remote branch and see his
changes reflected there. The rebase command is very versatile since it allows us to modify previous commits in our
repository.</li>
  <li><strong>reset</strong>: Set your branch to any commit in your repository. At this point a <strong>VERY IMPORTANT NOTE</strong> must be mentioned:
any repository, including the local one, stores commits. This means that commits <em>ALWAYS</em> survive, they are always
present, and one can always reset a branch head to a particular commit if the commit hash is available. It does not
matter that the commit had been “lost” in the branch chain.</li>
  <li><strong>cherry-pick</strong>: One can also cherry-pick a commit in the repository, given its hash, from any branch and apply it
to any other branch. Again, it does not matter that the picked commit does not currently appear on any branch, it
suffices for it to have been stored in the repository at some point.</li>
  <li><strong>log</strong>: This command is useful since it shows the chain of commits we have on our <em>local</em> repository that end up
in our current state. This does not show all the commits that once belonged to a branch if the branch was reset and
then new commits arrived (in a similar way to when we undo something in MS Word and then write new stuff, we cannot
redo). Useful for rebasing stuff.</li>
  <li><strong>reflog</strong>: This is <strong>ALL</strong> the repository history, including committed stuff, branch changes, pulled changes, etc.
You can find <strong>EVERYTHING</strong> here, even commits that were apparently “lost” from a branch. If you feel you have lost
a commit, just issue <code>git reflog</code> and cherry-pick from there.</li>
</ul>

<h3 id="my-workflow">My workflow</h3>

<p>When you are developing a new feature for your project, want you want to do is branch the master branch (or any other,
but it makes sense to pick up the official state of the project and start from there).</p>

<p><code>bash 
git checkout -b newfeature
</code></p>

<p>This creates a <strong>local branch</strong>. We are interested in storing our changes remotely so that if something happens to our
local copy (major Git messup that is easy to just ignore and pull the remote, hard drive issues, computer issues,
get your stuff done in some other computer, etc), we have our changes in the remote server too.</p>

<p><code>bash
git push origin newfeature
git branch --set-origin=origin/newfeature
</code></p>

<p>This creates the <strong>remote branch</strong> and sets the default link to our local branch, so that we can push to it.</p>

<p>After this, we can do our changes. I normally spend some time with a feature and by the time I want to commit to master (when the changes are not enough
to be peer reviewed in a pull request to the master branch), some other workmate has committed something, which forces
me to <strong>pull</strong> and <strong>rebase</strong>.</p>

<p>Imagine that I am working on newfeature and I notice that master has changed. It is better to incorporate the remote
changes sooner so that conflicts are minimal. I first <strong>stash</strong> my local changes, since I will need to change branch
to pull master (you can do it from newfeature branch, but I like it this way).</p>

<p><code>bash
git stash
git checkout master
git pull
</code></p>

<p>Now we have the remote latest changes. We switch to the newfeature branch</p>

<p><code>bash
git checkout newfeature
git rebase -i master
</code></p>

<p>This tells git to get (local, which was just updated) commits onto the current branch, newfeature. Now newfeature is
updated with the latest master state. We can keep on working. Get our latest changes, the ones we stashed.</p>

<p><code>bash
git stash apply
</code></p>

<p>This applies the top of the stash to the current clean local branch, and does not remove them from the stash. I am
normally interested in keeping these changes just in case. I can later remove them with <code>git stash drop</code>. I sometimes
use <code>git stash pop</code> which applies changes <em>and</em> removes them from the stash. If I need to refer to the stash to get
a change that I know was there but am not sure it is on the top, then I use <code>git stash show --name-only</code>, which
gives me the names of the changed files.</p>

<p>When I feel I can commit (because I am done or because I want to store remotely)</p>

<p><code>bash
git commit -m "Message"
git push origin newfeature
</code></p>

<p>Which saves my commits in the remote branch (remote repository).</p>

<p>I sometimes need to modify something that I committed, because I forgot to include it (and we want our master history
to be clean of mistakes and easy to follow), I prepare my local changes (tracked files) for commit fixup. First I use
the <code>git add</code> command to add these files to the subsequent commit. This commit will be done in a special manner:
using the <code>--fixup=</code> modifier, which accepts the hash of the commit to be modified. This is an extremely versatile
modifier, since it allows us to modify commits buried deep beneath our current head. This creates a new commit chain
with the fixed commit and all the commits that followed it, with new versions adapted to the changes we have introduced
(and of course, all with new hashes, since they are all new commits).</p>

<p>I came across a very easy command pipe to do this, since adding files one by one is a time waster:</p>

<p><code>bash
git status | grep modified | cut -d: -f2 | xargs git add
</code></p>

<p>The command <code>status</code> gives us the modified files, among other stuff (untracked files, etc). I filter the modified files
(getting only the file name) and then add them as an argument to <code>git add</code>.</p>

<p>We now fix the particular commit. In this case, I get the commit hash with <code>git log</code>.</p>

<p><code>bash
git commit --fixup=24ee1df941679953529aabc3d0eef5727a44c094
</code></p>

<p>This creates a <em>new</em> commit on top of the rest. We need to squash everything so that git creates the new commit chain,
wich entirely different commits and with the fixing and fixed commits merged into one. If the commit is buried below
the head, you would use the commit previous to the one being fixed. Imagine this chain</p>

<p><code>bash
c0cf52f4a914641efdc635bbc6195b6bef084cdc Categories and other plugin stuff
021086446698443ee57858872e14e7ee8ca9e2e5 Remove deploy stuff that got added by mistake
24ee1df941679953529aabc3d0eef5727a44c094 First commit to my-source
7f49807d5c5651baccb6d66c504549713a7ae7d7 Site updated at 2015-01-05 15:12:45 UTC
</code></p>

<p>When you create the fixup commit, it will appear on top of the chain</p>

<p><code>bash
f36806a856994c8365e199a72501a5461ca99497 fixup! First commit to my-source
c0cf52f4a914641efdc635bbc6195b6bef084cdc Categories and other plugin stuff
021086446698443ee57858872e14e7ee8ca9e2e5 Remove deploy stuff that got added by mistake
24ee1df941679953529aabc3d0eef5727a44c094 First commit to my-source
7f49807d5c5651baccb6d66c504549713a7ae7d7 Site updated at 2015-01-05 15:12:45 UTC
</code></p>

<p>Then you autosquash the chain, selecting the commit previous to the fix</p>

<p><code>bash
git rebase -i --autosquash 7f49807d5c5651baccb6d66c504549713a7ae7d7
</code></p>

<p>The result is a new chain</p>

<p><code>bash
c927d675189eedd89d972663e2ef0ffe1438f4f7 Categories and other plugin stuff
e08ef8e27b3fd3c15e07a5e032e66a67a533ca3d Remove deploy stuff that got added by mistake
7d94aace9629272848382abfc217282c676f108e First commit to my-source
7f49807d5c5651baccb6d66c504549713a7ae7d7 Site updated at 2015-01-05 15:12:45 UTC
</code></p>

<p>Where the new <code>7d94aace9629272848382abfc217282c676f108e</code> contains the modifications needed.</p>

<p>In case the commit we want to fix is on top of the chain, the autosquash is simpler:</p>

<p><code>bash
git rebase -i --autosquash HEAD~2
</code></p>

<p>If I had pushed my branch to the remote repository, then I would have to force push, since there is no way my local
branch and the remote one are compatible. To do that, you need to be pretty sure nobody is in the process of
committing anything to that particular branch (this is not a problem if you are working on this feature on your own),
and issue</p>

<p><code>bash
git push origin newfeature -f
</code></p>

<p>Where origin is my remote repository name and <code>-f</code> overwrites the remote contents.</p>

<h3 id="final-thoughts">Final thoughts</h3>

<p>We can see that we can modify our commit chain (resetting our branch head or fixing commits). Some people say it changes
the history, but I don’t see it that way). The true history is in the <strong>reflog</strong>.</p>

<p>Also, people might find it difficult to think of the usefulness of having several remote repositories. For example, this
blog has a remote which is, obviously, Github pages, and it is where the stuff under <code>_deploy</code> gets sebt.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[IPython Notebook for Data Analysis]]></title>
    <link href="http://analyticbastard.github.io/blog/2014/12/14/ipython-notebook-for-data-analysis/"/>
    <updated>2014-12-14T11:47:29+01:00</updated>
    <id>http://analyticbastard.github.io/blog/2014/12/14/ipython-notebook-for-data-analysis</id>
    <content type="html"><![CDATA[<p>A nice tool for both exploratory analysis and teaching is IPython Notebook. It consists of a web server executing
Python commands on an IPython interpreter and a set of Javascript files and CSS style sheets to layout the input and
output correctly and nicely. By connecting to the URL where the server is listening, you access to a book of pages,
each of one contains data input lines in Python and their corresponding beautified output, which can include images
and charts generated by <em>Matplotlib</em> or any other library that produces graphical output.</p>

<p>In the following, I wanted to face a recurrent problem that I have been facing in the past, derived from using
general tools such as Matlab for data analysis tasks. The problem with general tools is that they are easy to grasp and
feel confident with them, maybe too confident. In the case of Matlab, I became too comfortable with the flexibility of
matrices, which allow you to get started quickly since you can easily move data blocks around, but then force you to 
implement your in-house algorithms for data munging, or at least turn you too lazy to look for them 
(in the spirit of “that’ll only take me half an hour and then I’ll devote my time to productive
coding”).</p>

<p>I felt picky today, so I opened up my IPython Notebook server. For data munging, as everything else, one must not
re-invent the wheel, but let oneself use one of the excellent libraries out there. Pandas is an excellent example of
data munging libraries. Here we are going to align two time series, and the problem is the same than in the previous
post, i.e., align two time series with different time indices, such as two stock prices belonging to different markets,
observing different holidays (see the full description and solution in my previous post).</p>

<p>To do that, we concluded that the alignment could be made by merging two Pandas’ DataFrame objects, each containing
a series data (which can be multidimensional, and whose cells will be mixed), which produces some <em>NA</em>s. Then we could
apply the <em>gap</em> <em>NA</em> filling policy, which took the last valid value on each series.</p>

<p>The result, prettified by IPy Notebook can be seen in the figure to the right (you can click on it, a pop up will show
thanks to the <a href="https://github.com/rayfaddis/octopress-BootstrapModal">Bootstrap Image Pop plugin</a>, of which I will
talk in the following post).</p>

<p><!-- Image -->
<a id="img-8" class="imgModal floatRight" href="#imgModal-8" data-toggle="modal">
  <img src="/images/ipython-notebook.png" width="342" height="193" title="Click for larger view." />
</a>
<div style="float: none;"></div>

<!-- Modal -->
<div class="modal fade" id="imgModal-8" tabindex="-1" role="dialog" aria-labelledby="imgModal-8Label" aria-hidden="true">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
      </div>
      <div class="modal-body">
        <img src="/images/ipython-notebook.png" width="1366" height="771" />
      </div>
      <div class="modal-footer">
        <button type="button" class="btn btn-primary" data-dismiss="modal">Close</button>
      </div>
    </div><!-- /.modal-content -->
  </div><!-- /.modal-dialog -->
</div><!-- /.modal --></p>

<p>As you can see, this makes it especially adequate for information propagation environments such as board presentations
of data analytics or classroom interactive teaching. In the latter case, it is easy to imaging students connecting to
the teacher’s server and inputting their commands, while the teacher corrects them when they are wrong. To update items
in real time, IPy Notebook should contemplate using websockets or any kind of server side events, which I believe
does not so far.</p>

<p>Anyway, it is a great tool to present results in a very neat way.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Future of Data Analysis Tools: Python, Web Technologies]]></title>
    <link href="http://analyticbastard.github.io/blog/2014/11/27/the-future-of-data-analysis-tools/"/>
    <updated>2014-11-27T12:01:19+01:00</updated>
    <id>http://analyticbastard.github.io/blog/2014/11/27/the-future-of-data-analysis-tools</id>
    <content type="html"><![CDATA[<p>For the past years I have observed a shift, or convergence one might say, in the tools used in several disciplines
involving data handling or processing. An easy example is this blog, which is made with 
<a href="http://octopress.org/">Octopress</a>, a tool heavily based on <em>nerdy</em> concepts such as compilation, version control,
modular building and Markdown syntax, and tools such as Git and the Ruby toolkit. This makes blogging more similar
to software development, as I hinted <a href="/blog/2014/11/01/my-new-blog/">on my first post on this blog</a>.</p>

<p>We find this shift to be a real convergence in the case of data analysis and software development, specifically,
web software development and scripting with Python. This is almost self evident and has been noted previously, in fact,
a blog entry talking about
<a href="http://www.talyarkoni.org/blog/2013/11/18/the-homogenization-of-scientific-computing-or-why-python-is-steadily-eating-other-languages-lunch/">pythonification of a scientist’s data toolkit</a>
and thinking about my own data analysis toolkit got me writing about this.</p>

<h3 id="the-past">The past</h3>

<p>Previously, I relied primarily on Matlab. Matlab mostly considers matrices as the primary building block in programs
(cells are another very useful structure to consider when elements of a set do not share the same dimension or types).
Everything is a matrix, a hyper-rectangle of things, from scalars to multi-dimensional matrices. This makes moving data
in blocks very easy, since this constitutes an atomic vectorial operation. However, this reduction to the general
case exposes several problems that one normally faces during the data analysis process. For example, one might be
interested in correlating sells with the social sentiment about our product. This involves scraping candidate web
pages, storing interesting parts of the text, performing sentiment analysis, aggregating by date and aligning with
sells by date. This is an unbearable task to do by moving data blocks in Matlab.</p>

<p>R, as in <a href="http://www.r-project.org/">GNU R-Project</a> is a more complete framework for data munging, since it seeks to
replicate the S statistical language in its origins. R, which has the concept of objects, defines a dataframe class
that specifically considers column as attributes and rows as instances, defines a set of methods that allow us to
deal with specialized tasks like sorting, filtering or rearranging. Although I have used R in the past, I never came to
like it despite the hught support by the community. I feel that some piece is missing, and that is maybe the heavily
typing for a data analysis language, poor language-level support for functional paradigms which are great for data
munging (map, reduce, filter and the like). Algorithms must be implemented in an imperative setting, which isn’t
sometimes the best option.</p>

<h3 id="the-present">The present</h3>

<p>Python comes in the middle of it all. It supports functional paradigms such as lambda functions (functions defined
where they are used), functions as first-class objects, and classical operations on collections. On the other hand,
Python has increasingly getting more support from the community and the amount of available libraries is astonishing.
It is true that the support for statistical analysis in Python cannot be compared to that or R yet, but Python is
steadily catching up, with ever increasing scientists switching in all disciplines. A number of mature projects
exist in all the areas, ranging from general Statistics to Neuroimaging. Several bridges exist to R packages with no
native counterpart. This, the language support for advanced features, optimized packages such as
<a href="www.lfd.uci.edu/~gohlke/pythonlibs/">Cristoph Golke’s</a> (which include NumPy and SciPy versions statically compiled
against the highly efficient Intel’s MKL BLAS distributions) and the extra toolset derived from years of using
Python as a generalist scripting data (including a highly-productive web toolset), makes Python a worthy
next-generation substitute for R.</p>

<p>Imagine that we collect two time series of stock prices that originated in two different markets. These two markets
observe different holidays, so the raw time series that we get are unaligned (this is true for Yahoo Finance historical
data, for example). Our data pre-processing task is to align the data, keeping the last valid price for holidays where
the other market is open.</p>

<p>Pandas is a Python library that includes much of R functionality and is extremely handy for this kind of data munging
tasks. By creating DataFrame objects from our raw data, we access the kind of functionality we need. In this case, once
we have two DataFrame objects, <em>dt1</em> and <em>dt2</em>, where we simulates two weeks of data, by starting on Monday, 1st,
and ending on Friday, 12th. The first market observes a holiday on Friday the 5th, while the second observes a
holiday on Monday, 1st. We define the dataframes as follows:</p>

<p><code>python
dt1 = DataFrame({'date' : [1,2,3,4,8,9,10,11,12], 'value' : [100,101,102,103,104,105,106,107,108]})
dt2 = DataFrame({'date' : [1,2,3,4,5,9,10,11,12], 'value' : [100,101,102,103,104,105,106,107,108]})
</code></p>

<p>To successfully mix the data toether, we can use the merge function on the column date, specifying the <em>outer</em> merging
method, which keeps data rows coming from both dataframes.</p>

<p><code>python
dt1.merge(dt2, on=['date'], how='outer')
</code></p>

<p>This will output</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th style="text-align: center">date</th>
      <th style="text-align: right">value_x</th>
      <th style="text-align: right">value_y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: right">100</td>
      <td style="text-align: right">100</td>
    </tr>
    <tr>
      <td>1</td>
      <td style="text-align: center">2</td>
      <td style="text-align: right">101</td>
      <td style="text-align: right">101</td>
    </tr>
    <tr>
      <td>2</td>
      <td style="text-align: center">3</td>
      <td style="text-align: right">102</td>
      <td style="text-align: right">102</td>
    </tr>
    <tr>
      <td>3</td>
      <td style="text-align: center">4</td>
      <td style="text-align: right">103</td>
      <td style="text-align: right">103</td>
    </tr>
    <tr>
      <td>4</td>
      <td style="text-align: center">8</td>
      <td style="text-align: right">104</td>
      <td style="text-align: right">NaN</td>
    </tr>
    <tr>
      <td>5</td>
      <td style="text-align: center">9</td>
      <td style="text-align: right">105</td>
      <td style="text-align: right">105</td>
    </tr>
    <tr>
      <td>6</td>
      <td style="text-align: center">10</td>
      <td style="text-align: right">106</td>
      <td style="text-align: right">106</td>
    </tr>
    <tr>
      <td>7</td>
      <td style="text-align: center">11</td>
      <td style="text-align: right">107</td>
      <td style="text-align: right">107</td>
    </tr>
    <tr>
      <td>8</td>
      <td style="text-align: center">12</td>
      <td style="text-align: right">108</td>
      <td style="text-align: right">108</td>
    </tr>
    <tr>
      <td>9</td>
      <td style="text-align: center">5</td>
      <td style="text-align: right">NaN</td>
      <td style="text-align: right">104</td>
    </tr>
  </tbody>
</table>

<p>We notice the dates are unsorted due to using the first dataframe, which skips friday (date 8 would be unsorted if we
were to use the <em>dt2</em> object). We sort on the <em>date</em> column. However, there is a bigger problem, we see NaNs,
Not a number values for column of a dataframe not defined in the other, since this is the usual outcome from the
outer merge. In this case, the <em>fillna</em> method of the DataFrame class with the <em>gap</em> policy will fit our purposes
(carry over the last valid value).</p>

<p><code>python
dt1.merge(dt2, on=['date'], how='outer').sort(columns=['date']).fillna(method='pad')
</code></p>

<p>The result of all steps is</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th style="text-align: center">date</th>
      <th style="text-align: right">value_x</th>
      <th style="text-align: right">value_y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: right">100</td>
      <td style="text-align: right">100</td>
    </tr>
    <tr>
      <td>1</td>
      <td style="text-align: center">2</td>
      <td style="text-align: right">101</td>
      <td style="text-align: right">101</td>
    </tr>
    <tr>
      <td>2</td>
      <td style="text-align: center">3</td>
      <td style="text-align: right">102</td>
      <td style="text-align: right">102</td>
    </tr>
    <tr>
      <td>3</td>
      <td style="text-align: center">4</td>
      <td style="text-align: right">103</td>
      <td style="text-align: right">103</td>
    </tr>
    <tr>
      <td>9</td>
      <td style="text-align: center">5</td>
      <td style="text-align: right">103</td>
      <td style="text-align: right">104</td>
    </tr>
    <tr>
      <td>4</td>
      <td style="text-align: center">8</td>
      <td style="text-align: right">104</td>
      <td style="text-align: right">104</td>
    </tr>
    <tr>
      <td>5</td>
      <td style="text-align: center">9</td>
      <td style="text-align: right">105</td>
      <td style="text-align: right">105</td>
    </tr>
    <tr>
      <td>6</td>
      <td style="text-align: center">10</td>
      <td style="text-align: right">106</td>
      <td style="text-align: right">106</td>
    </tr>
    <tr>
      <td>7</td>
      <td style="text-align: center">11</td>
      <td style="text-align: right">107</td>
      <td style="text-align: right">107</td>
    </tr>
    <tr>
      <td>8</td>
      <td style="text-align: center">12</td>
      <td style="text-align: right">108</td>
      <td style="text-align: right">108</td>
    </tr>
  </tbody>
</table>

<p>Of course, we have excellent IDEs to work with, ranging from generalist tools and plugins for major IDE frameworks
such as Eclipse and IntelliJ IDEA, to more specialized editors such as the Matlab-like Spyder, IPython Notebook (embedded
in the IPython executable, which can be started with the option <code>-notebook</code>). Also, emerging editors such as the
celebrated Sublime Text and Light Table fully support Python.</p>

<h3 id="data-visualization-past-and-present">Data visualization: past and present</h3>

<p>An often overlooked side is data visualization. As people coming from the engineering side, where doing ugly and
hard to use things is almost a badge of pride, we never care about how our results look as long as they are correct
(in a sense that they fulfill their functional requirements). Presentation is not only important from the aesthetic
point of view, but can also help the experts discover patterns in the data that offer previously unknown clues about
the nature of our data.</p>

<p>Plotting Matlab graphs and pasting them on a Word document was the normal. R has an impressive set of graphical tools
that cope with the most exigent user. For the presentations, using PowerPoint was the thing to do in corporate
environments and the most adventurous could embark on the quest of using Latex Beamer for this purpose. Not any more.</p>

<p>The new normal will be web technologies. Web browsers are the most advanced graphical tool available to any user in the
world. The dynamic capabilities achieved by both CSS and Javascript make any other technology pale. To easy the burden
of dealing with raw CSS and Javascript, a number of libraries have been built on top of the browser native support.
Some of these rise above the others. I must mention <a href="http://d3js.org">D3js</a>, an impressive graphical library with
anything a data scientist with graphical needs might need. Visit their examples page to get a glance of the
capabilities.</p>

<iframe width="720" height="480" marginheight="-500" marginwidth="-300" src="http://mbostock.github.io/d3/talk/20111018/collision.html"></iframe>

<p>Lastly, PowerPoint and Latex Beamer users with professional needs will both shift to browser presentation technologies
such as <a href="http://lab.hakim.se/reveal-js/#/">RevealJS</a>, which can be complemented with Latex renderers to achieve perfect
results for the mathematical formulae, on top of superior interactions.</p>

<iframe width="720" height="480" src="http://lab.hakim.se/reveal-js/#/"></iframe>

<h3 id="the-future">The future</h3>

<p>A large shift towards Python can be expected, especially in rapid prototyping. Even though R is not going away soon,
Python bindings will relegate R to a second class language, so to speak, in the same way as Fortran is today, this is,
there are many classical algorithms written in Fortran in the 70’s and 80’s still being in production today. This
includes a huge number of well-known and widely-used linear algebra libraries such as Netlib’s BLAS and LAPACK, which
are currently interfaced to other languages such as R.</p>

<p>Also, functional programming should play a role in data analysis. Lisps variants have an important advantage over imperative
paradigms such as working naturally with monads. This makes the use of the monad <em>some</em> very useful for list processing.
For example, in closure we can write</p>

<p>&#8220;` clojure
(def data [1 2 3 4])</p>

<p>(some-&gt; data
        process1
        process2
        process3)
&#8220;`</p>

<p>which prevents us from using a large chain of nested if blocks for this conditional processing. However, Lisp syntax
is not well suited for rapid prototyping, where data scientists prefer a more imperative approach to define global
variables for later processing</p>

<p><code>python
X = np.array([1,2,3,4])
</code></p>

<p>Functional languages might become, if not a rapid prototyping choice, indeed a data system deployment choice, since
a lot more data processing and state handling will need to be done. It is worth mentioning
<a href="http://clojure.github.io/clojure/clojure.zip-api.html">zippers</a>,
<a href="https://clojure.github.io/clojure/clojure.walk-api.html">walkers</a> and
<a href="https://github.com/clojure/core.match">match</a>, three libraries that make the programmer’s life easier by orders of
magnitude when dealing with data processing, but I will devote an article to them.</p>

<p>Regarding data visualization, it will be done primarily with web technologies on the browser, with AJAX data requests
to the server were the data processing and storage are done. Here, technologies such as D3js will be a must.</p>

<h3 id="summary">Summary</h3>

<p>In the future, it is conceivable to use an integrated framework that analyzes data with Python libraries and then
presents the results via a Python web server to multiple browsers.</p>

<p>As well as Java will not totally go away in the enterprise software development world, neither will current data processing
technologies such as Excel, PowerPoint, Matlab or R, since they have die hard niches. In particular, the huge amount of
existing libraries will need time to be adapted to Python.</p>

]]></content>
  </entry>
  
</feed>
