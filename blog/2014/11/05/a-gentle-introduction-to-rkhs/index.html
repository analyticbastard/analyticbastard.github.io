
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>A Gentle Introduction to RKHS and Kernel Methods - The Analytic Bastard</title>
  <meta name="author" content="Analytic Bastard">

  
  <meta name="description" content="A (real or complex) Reproducing kernel Hilbert spaces (RKHS), is a Hilbert space of point-defined functions where the evaluation functional is linear &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://analyticbastard.github.io/blog/2014/11/05/a-gentle-introduction-to-rkhs">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="The Analytic Bastard" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.8.16/jquery-ui.min.js" type="text/javascript"></script>
  <script src="//netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib<n/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<script type="text/javascript" src="https://www.google.com/jsapi"></script>
<script type="text/javascript">google.load("feeds", "1");</script>

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">The Analytic Bastard</a></h1>
  
    <h2>Making geometers go crazier</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:analyticbastard.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">A Gentle Introduction to RKHS and Kernel Methods</h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2014-11-05T00:31:32+01:00'><span class='date'><span class='date-month'>Nov</span> <span class='date-day'>5</span><span class='date-suffix'>th</span>, <span class='date-year'>2014</span></span> <span class='time'>12:31 am</span></time>
        
      </p>
    
  </header>


<div class="entry-content"><p>A (real or complex) Reproducing kernel Hilbert spaces (RKHS), is a Hilbert space of point-defined functions where the evaluation functional is linear and bounded (equivalently continuous). That the functions are point-wise defined is almost self explanatory, and means that the objects in the space, the functions, are built from defining them at locations within a domain (a compact domain). The way of associating the function to its value at a location is done via the evaluation functional. Think of the evaluation functional <em>at a location</em> as a black box (not so black box in the RKHS) that, given a function as an argument, spits the function value at that location. The fact that the evaluation functionals are linear and bounded roughly means that if an evaluation functional evaluates the sum of two functions (the sum in their vector space), then the results amounts to summing the two evaluations of both functions by the linear functional, i.e.,
<script type="math/tex">
\delta_x(\alpha f + \beta g) = \alpha \delta_x(f) + \beta \delta_x(g)
</script>
for a location $x$, and two real (or complex) numbers $\alpha$ and $\beta$.</p>

<p>This theory was originally developed as a functional analytic abstraction to solve a linear differential equations (as were Hilbert and Banach spaces) of positive (or negative) kind, but made their way to data analysis first in the theory of splines and later became the engine of multiple abstract machines, based on more general reproducing kernels.</p>

<p>Let $\mathbf{X}$ be a compact set or a manifold. The linear property of the evaluation functional implies, by the Riesz representation theorem, that it has a representer within the space, in contrast, for example, to the Dirac’s $\delta$ evaluation functional of the Hilbert space of squared-integrable functions (equivalence classes of functions) $L^2(\mathbf{X})$, which is a generalized function and, thus, does not belong to $L^2(\mathbf{X})$. 
The Riesz representation theorem implies that there is an element <em>within</em> the space of functions that yields the same results when operating it with the rest of the elements of that space than a certain linear functional. Note that this, alone, does not say that all functionals that are evaluation functionals are linear. 
It is only when they are linear that the Riesz theorem applies and they can be associatited to certain elements within the space, and we find ourselves with a RKHS.</p>

<p>A function $f$ belonging to a RKHS $H_k$ can then be evaluated, at any point $\mathbf{x}$ in the set on which the functions in $H_k$ are defined, with the <em>reproducing property</em>
<script type="math/tex">
f(\mathbf{x}) = \delta_{\mathbf{x}} (f) = \langle k_{\mathbf{x}}, f \rangle_{H_k}
</script>
where we call $\delta_x$ to the linear evaluation functional at location $\mathbf{x}$ for $H_k$, belonging to $H_k^{*}$, the algebraic dual of $H_k$, whose representer element in $H_k$ is $k_x$. The notation $k_x = k(\cdot, x) \in H_k$ means that we fix the second argument to $\mathbf{x}$ so that $k$ is a function only on the first argument, and then it belongs to the very space $H_k$ it can reproduce pointwise via inner products. We work with real RKHS henceforth.</p>

<p>This function with two arguments, $k:\mathbf{X} \times \mathbf{X} \rightarrow \mathbb{R}$, is the <em>kernel</em> of the positive linear integral operator 
<script type="math/tex">
T_k : L_X^2(\mathbf{X}) \rightarrow H_k
</script>
such that the bi-linear form
<script type="math/tex">
\langle f, T_k f \rangle_{L^2(\mathbf{X})} = \int_{\mathbf{X}} \int_{\mathbf{X}} k(\mathbf{x}_1,\mathbf{x}_2) f(\mathbf{x}_2) dP_X(\mathbf{x}_2) f(\mathbf{x}_1) dP_X(\mathbf{x}_1)
</script>
is positive, where $P_X$ is a finite Borel measure endowing $\mathbf{X}$ and $f \in L_{P_X}^2(\mathbf{X})$, the Hilbert space of square-integrable functions under measure $P_X$. Such a kernel is called positive-definite.
This of this as the infinite-dimensional equivalent of a matrix, which instead of finite dimensional vectors, operates functions in a linear fashion (the kernel itself does not depend on the function it is operated with under the integral sign). This is the functional-analytic way of solving differential equations, since we can invert the linear differential operator with these kind of integral operators, and this get the solution. </p>

<p>A particularly important class of linear operator kernels are positive-definite kernels, and of particular interest is that of Mercer’s kernels. Given a Mercer’s kernel $k$, it holds that
<script type="math/tex">
k(\mathbf{x}_1,\mathbf{x}_2)=\Phi(\mathbf{x}_1)^T\Phi(\mathbf{x}_2) = \sum_{j=1}^{\infty} \lambda_j \phi_j(\mathbf{x}_1) \phi_j(\mathbf{x}_2)
</script>
where $\lambda_j$ and $\phi_j$ are the eigenvalues and eigenfunctions of the linear integral operator $T_k$, which is compact. This iduces a map $\Phi: \mathbf{X} \rightarrow \ell^2$, the space of square-summable sequences. This map $\Phi = (\sqrt{\lambda_1} \phi_1, \sqrt{\lambda_2} \phi_2, \ldots)^T$ is the so-called <em>kernel embedding</em> and allows the practitioner to map the data to a definite dimension (possibly infinite), where the learning task is likely to become linear. This map, however, needs not be computed explicitly in Kernel Learning methods and is defined, as seen above, by the kernel we are using and, in practice, we are restricted to a finite number of training points. This map can be written as a vector $\mathbf{z} \in \mathbb{R}^D$ for each datum, with a dimension $D \leq N$. The collection for $N$ data can be written as a matrix $\mathbf{Z} \in \mathbb{R}^{N \times D}$ of vectors $\mathbf{z}$ vertically stacked, and the finite version of the Mercer kernel expansion, yields the finite-dimensional embedding $\mathbf{Z}$ and the so-called <em>kernel trick</em>
<script type="math/tex">
\mathbf{K} = \mathbf{Z} \mathbf{Z}^T
</script>
where $\mathbf{K} \in \mathbb{R}^{N \times N}$ is the Gram matrix of kernel evaluations on the dataset <script type="math/tex">\{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N\}</script> built so that each entry of the matrix correspond to the kernel evaluation at two points, $\mathbf{K}_{i,j} = k(\mathbf{x}_i, \mathbf{x}_j)$. This embedding constituted the breakthrough in Machine Learning in the late 90s and early new century. Notice that this feature space is <strong>not</strong> the RKHS, since it is a subspace of $\ell^2$, their elements being sequences, whereas the RKHS is a subspace if $L^2$, its elements being functions.</p>

<p>Let $f,g \in H_k$ be expressed as a linear combination of the eigenfunctions of $T_k$, <script type="math/tex">f = \sum_{j=1}^{\infty} \alpha_j \phi_j</script> and <script type="math/tex">g = \sum_{j=1}^{\infty} \beta_j \phi_j</script>. The inner product in $H_k$ is defined as
<script type="math/tex">
\langle f,  g \rangle_{H_k} = \sum_{j=1}^{\infty} \frac{\alpha_j \beta_j}{\lambda_j}
</script>
Which is equal to the equation above, i.e., 
<script type="math/tex">\langle f,  g \rangle_{H_k} = \langle f, T_k g \rangle_{L^2(\mathbf{X})}
</script>.
This induces a norm
<script type="math/tex">
\|f\|_{H_k}^2 = \langle f,  f \rangle_{H_k} =  \|P f\|_{L^2(\mathbf{X})}^2
</script>
Where $P$ is a pseudo-differential operator and <script type="math/tex">P^{*}</script> is its adjoint such that <script type="math/tex">T_k = P^{*} P</script>.</p>

<p>The learning task in kernel methods consist on computing a function that can approximate certain pre-defined data with certain desired properties, namely, that it is “simple” enough, simplicity measured by the norm in the space, the smaller norm, the better, as this requirement tries to overcome the overfitting and noise presence (the less “wiggly” the function, the less it responds to noise). So given a kernel function $k$ and a finite data sample of size $N$, the Representer theorem ensures that the subspace of functions spanned by finite linear combinations of the form $f=\sum_j^{n}{\alpha_j k(\cdot, \mathbf{x}_j)}$ is the solution to a regularization problem of the form
<script type="math/tex">
\min_{f \in H_k}  \sum_{j=1}^{n} (y_i - f(\mathbf{x}_i))^2 + \lambda \|f\|_{H_k}^2
</script>
where $\lambda$ is the regularization parameter, and where we see that both the approximation results on our existing training data and the complexity of the function are accounted for. This kind of quadratic functionals appear in multiple kernel methods, including support vector machines (SVM). These quadratic functionals are the ones optimized by any quadratic optimization method to arrive at an acceptable solution.</p>

<p>Despite the proven power of kernel methods, they have a main drawback, which is that they scale with the square of the number of data $N$. Managing matrices larger than $N&gt;10000$ is unmanageable for most computers, and the $N^2$ scaling renders supercomputers unable to handle these matrices. To partially overcome this difficulty, several methods have been developed. Firstly, the Nyström method can be used to approximate any kernel Gram matrix simply using the fact that for an integral linear operator as in the above equation, the eigenequation
<script type="math/tex">
\int_{\mathbf{X}} k(\mathbf{x}_1,\mathbf{x}_2) \phi_i(\mathbf{x}_2) dP_X(\mathbf{x}_2) = \lambda_i \phi_i(\mathbf{x}_1)
</script>
holds, so that uniformly sampling $\mathbf{x}$ from $P_X$, we can make the approximation of the kernel matrix
<script type="math/tex">
\mathbf{K} \mathbf{u} = \lambda \mathbf{u}
</script>
with the restricted subsample of size $q$, where $\mathbf{u}$ is an eigenvector approximation. Then, one can obtain an approximation to the first eigenvectors of the matrix, which have the following expressions
<script type="math/tex">
\phi_i(\mathbf{x}) \approx \sqrt{q} \mathbf{u}_i^{(q)} \quad\quad \lambda_i \approx \frac{\lambda_i^{(q)}}{q}
</script></p>

<p>Another well-known approximation is the random features technique, this time only for translation invariant kernels (i.e., those which can be written <script type="math/tex">k(\mathbf{x}_1,\mathbf{x}_2)=k(\mathbf{x}_1-\mathbf{x}_2)</script>, abusing notation and writing $k$ for both functions) was developed by Rahimi and Rech. Bochner’s theorem is a result from classical harmonic analysis and applies to the mentioned kernel types. A continuous function <script type="math/tex">k \in L^1(\mathbb{R}^N)</script> is positive definite if and only if it is the Fourier transform of a non-negative measure $\Lambda$.
<script type="math/tex">
k(\mathbf{x}_1-\mathbf{x}_2)=\int_{\mathbb{R}^N}{e^{i\mathbf{\omega}^T (\mathbf{x}_1-\mathbf{x}_2)}d \Lambda(\mathbf{\omega})}
</script>
The method, then, consist on sampling (multivariate) frequencies $\mathbf{\omega}$ from the probability distribution $\Lambda$ related to the kernel $k$ and build feature vectors from the Fourier complex exponentials $e^{-i\mathbf{\omega}^T \mathbf{x}}$, pairs of sines and cosines at frequency $\mathbf{\omega}$, or using only a cosine at frequency $\mathbf{\omega}$ with phase $b$ sampled from a uniform distribution between zero and $2\pi$. The kernel approximation at a point is, then, the product of all the features at that point. Comparison between both methods have been made and it has been found that Nyström method has a number of advantages, such as approximating speed, stemming mainly from the dependence on the distribution of the data $P_X$, whereas random features proceed independently of $P_X$. This idea has also been used in Gaussian Processes with success.</p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Analytic Bastard</span></span>

      




<time class='entry-date' datetime='2014-11-05T00:31:32+01:00'><span class='date'><span class='date-month'>Nov</span> <span class='date-day'>5</span><span class='date-suffix'>th</span>, <span class='date-year'>2014</span></span> <span class='time'>12:31 am</span></time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/kernel-methods/'>kernel methods</a>, <a class='category' href='/blog/categories/machine-learning/'>machine learning</a>, <a class='category' href='/blog/categories/mathematics/'>mathematics</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://analyticbastard.github.io/blog/2014/11/05/a-gentle-introduction-to-rkhs/" data-via="" data-counturl="http://analyticbastard.github.io/blog/2014/11/05/a-gentle-introduction-to-rkhs/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2014/11/03/improve-the-performance-of-an-svm-with-differential-geometry/" title="Previous Post: Improve the performance of an SVM with differential geometry">&laquo; Improve the performance of an SVM with differential geometry</a>
      
      
        <a class="basic-alignment right" href="/blog/2014/11/05/naive-bayes-implemented-with-map-slash-reduce-operations/" title="Next Post: Naive Bayes implemented with Map/Reduce operations">Naive Bayes implemented with Map/Reduce operations &raquo;</a>
      
    </p>
  </footer>
</article>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2015/03/08/the-monty-hall-problem-my-explanation/">The Monty Hall Problem (My Explanation)</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/26/powering-up-python-as-a-data-analysis-platform/">Powering Up Python as a Data Analysis Platform</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/20/image-pan-sharpening-with-pca/">Image Pan-sharpening With PCA</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/08/google-original-page-rank/">Google&#8217;s Original Page Rank Implementation</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/01/10/my-git-workflow/">My Git Workflow</a>
      </li>
    
  </ul>
</section>
<section>
    <h1>Categories</h1>
    <ul id="category-list"><li><a href='/blog/categories/brainteasers'>brainteasers (1)</a></li><li><a href='/blog/categories/computation'>computation (5)</a></li><li><a href='/blog/categories/computing'>computing (1)</a></li><li><a href='/blog/categories/data-analysis'>data analysis (4)</a></li><li><a href='/blog/categories/hacking'>hacking (1)</a></li><li><a href='/blog/categories/image-processing'>image processing (1)</a></li><li><a href='/blog/categories/kernel-methods'>kernel methods (3)</a></li><li><a href='/blog/categories/machine-learning'>machine learning (4)</a></li><li><a href='/blog/categories/mathematics'>mathematics (4)</a></li><li><a href='/blog/categories/maths'>maths (1)</a></li><li><a href='/blog/categories/matlab'>matlab (1)</a></li><li><a href='/blog/categories/matrices'>matrices (1)</a></li><li><a href='/blog/categories/programming'>programming (5)</a></li><li><a href='/blog/categories/python'>python (3)</a></li><li><a href='/blog/categories/software-development'>software development (1)</a></li><li><a href='/blog/categories/statistics'>statistics (4)</a></li><li><a href='/blog/categories/tools'>tools (4)</a></li></ul>
</section>
<section>
    <h1>Tag Cloud</h1>
    <span id="tag-cloud"><a href='/blog/categories/brainteasers' style='font-size: 112.0%'>brainteasers</a> <a href='/blog/categories/computation' style='font-size: 160.0%'>computation</a> <a href='/blog/categories/computing' style='font-size: 112.0%'>computing</a> <a href='/blog/categories/data-analysis' style='font-size: 148.0%'>data analysis</a> <a href='/blog/categories/hacking' style='font-size: 112.0%'>hacking</a> <a href='/blog/categories/image-processing' style='font-size: 112.0%'>image processing</a> <a href='/blog/categories/kernel-methods' style='font-size: 136.0%'>kernel methods</a> <a href='/blog/categories/machine-learning' style='font-size: 148.0%'>machine learning</a> <a href='/blog/categories/mathematics' style='font-size: 148.0%'>mathematics</a> <a href='/blog/categories/maths' style='font-size: 112.0%'>maths</a> <a href='/blog/categories/matlab' style='font-size: 112.0%'>matlab</a> <a href='/blog/categories/matrices' style='font-size: 112.0%'>matrices</a> <a href='/blog/categories/programming' style='font-size: 160.0%'>programming</a> <a href='/blog/categories/python' style='font-size: 136.0%'>python</a> <a href='/blog/categories/software-development' style='font-size: 112.0%'>software development</a> <a href='/blog/categories/statistics' style='font-size: 148.0%'>statistics</a> <a href='/blog/categories/tools' style='font-size: 148.0%'>tools</a> </span>
</section><section>
    <script type="text/javascript" src="//rb.revolvermaps.com/0/0/6.js?i=1eyn18efppk&amp;m=7&amp;s=270&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script>
</section><section>
  <h3>Blogroll</h3>
  <!-- IMPORTANT: Do not change the below ID as it will affect the functioning of the plugin-->
    <ul id="blogroll-list"><li><a href='http://blog.cognitect.com/' feedurl='http://blog.cognitect.com/blog?format=rss' title = 'Clojure, ClojureScript, Datomic, Programming'>Cognitec</a></li><li><a href='http://www.datasciencecentral.com/' feedurl='http://feeds.feedburner.com/FeaturedBlogPosts-DataScienceCentral?format=xml' title = 'Big Data, Data Mining, Machine Learning'>Data Science Central</a></li><li><a href='http://www.datasciencecentral.com/' feedurl='http://feeds.feedburner.com/ResearchDiscussions-DataScienceCentral?format=xml' title = 'Big Data, Data Mining, Machine Learning'>Data Science Central</a></li><li><a href='http://deeplearning.net' feedurl='http://deeplearning.net/feed/' title = 'Deep Learning, Neural Networks'>deeplearning.net</a></li><li><a href='http://hunch.net' feedurl='http://feeds2.feedburner.com/MachineLearningtheory' title = 'Machine Learning, Data Mining'>Hunch.net</a></li><li><a href='http://blog.kaggle.com' feedurl='http://blog.kaggle.com/feed/' title = 'Data mining, Big Data, Machine Learning, Competitions'>Kaggle</a></li><li><a href='http://www.kdnuggets.com/' feedurl='http://feeds.feedburner.com/kdnuggets-data-mining-analytics?format=xml' title = 'Data Mining, Big Data, Machine Learning'>KDnuggets</a></li><li><a href='http://www.thoughtworks.com/' feedurl='http://feeds.feedburner.com/thoughtworks-blogs?format=xml' title = 'Programming, Software Engineering'>Thoughtworks</a></li><li><a href='http://engineeringblog.yelp.com' feedurl='http://engineeringblog.yelp.com/feed' title = 'Big Data, Machine Learning, Text Mining'>Yelp Engineering</a></li></ul>
    
    <script type="text/javascript">
      $(document).ready(function(){
        updateFeeds(true);
      });
    </script>
    <script src="/javascripts/tinysort-min.js"></script>
    <script src="/javascripts/blogroll.js"></script>
    
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/analyticbastard">@analyticbastard</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'analyticbastard',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2015 - Analytic Bastard -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>






<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

</body>
</html>
