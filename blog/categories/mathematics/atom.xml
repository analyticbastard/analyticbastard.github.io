<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Mathematics | The Analytic Bastard]]></title>
  <link href="http://analyticbastard.github.io/blog/categories/mathematics/atom.xml" rel="self"/>
  <link href="http://analyticbastard.github.io/"/>
  <updated>2015-03-18T01:39:05+01:00</updated>
  <id>http://analyticbastard.github.io/</id>
  <author>
    <name><![CDATA[Analytic Bastard]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Principal Component Analysis]]></title>
    <link href="http://analyticbastard.github.io/blog/2015/01/01/principal-component-analysis/"/>
    <updated>2015-01-01T01:22:01+01:00</updated>
    <id>http://analyticbastard.github.io/blog/2015/01/01/principal-component-analysis</id>
    <content type="html"><![CDATA[<p>Principal Component Analysis is a classical statistical technique that aims at finding a transformation of the
input or measured variables so that the transformed variables offer a view of the data that maximizes the presented
“information” about the dataset. This allows for dimensionality reduction, since we can select a number of dimensions
which provide most of the “information” and be sure that each of the rest of the discarded dimensions contain less
information than each of the dimensions we have retained.</p>

<p>We start by requesting something as simple as our solution to be a constrained linear combination (so that the
coefficient vector is of norm one) so that it is of maximum square norm.</p>

<script type="math/tex; mode=display">
\max_{\|\mathbf{w}\|=1}  || \mathbf{X} \mathbf{w} ||^2 \\
\mbox{s.t. }  \mathbf{w}^T \mathbf{w_i} = 0
</script>

<p>Maximizing the square norm explains why the first component of non-centered data contains some kind of a data average
for example, the first <a href="http://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html">eigenface</a> .</p>

<p><!-- Image -->
<a id="img-7" class="imgModal floatRight" href="#imgModal-7" data-toggle="modal">
  <img src="/images/plot_faces_decomposition_002.png" width="300" height="226" title="Click for larger view." />
</a>
<div style="float: none;"></div>

<!-- Modal -->
<div class="modal fade" id="imgModal-7" tabindex="-1" role="dialog" aria-labelledby="imgModal-7Label" aria-hidden="true">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
      </div>
      <div class="modal-body">
        <img src="/images/plot_faces_decomposition_002.png" width="600" height="451" />
      </div>
      <div class="modal-footer">
        <button type="button" class="btn btn-primary" data-dismiss="modal">Close</button>
      </div>
    </div><!-- /.modal-content -->
  </div><!-- /.modal-dialog -->
</div><!-- /.modal --></p>

<p>where, in the case of the first principal component, there is no $i$ less than one and the constraint does not apply.</p>

<p>Since the scalar product induces a norm, $ \mathbf{w}^T \mathbf{w}$ is the norm of $\mathbf{w} $ and the above becomes</p>

<p>Let $\mathbf{X}$ be the data matrix, let $\mathbf{\Sigma} = \mathbf{X}^T \mathbf{X}$ be the covariance matrix.</p>

<script type="math/tex; mode=display">
\max_{\|\mathbf{w}\|=1}  \mathbf{w}^T \mathbf{\Sigma} \mathbf{w} \\
\mbox{s.t. }  \mathbf{w}^T \mathbf{w_i} = 0
</script>

<p>For the first principal component, this is an quadratical optimization problem constrained to unitary norm of the
solution. This can be written as the optimization of a Rayleigh quotient.</p>

<script type="math/tex; mode=display">
\max_{\mathbf{w}} \frac{\mathbf{w}^T \mathbf{\Sigma} \mathbf{w}}{||\mathbf{w}||^2} \\
\max_{\mathbf{w}} \frac{\mathbf{w}^T \mathbf{\Sigma} \mathbf{w}}{\mathbf{w}^T \mathbf{w}} \\
</script>

<p>This is a Rayleigh quotient and it is well known from Matrix Analysis that the solution is in terms of the eigenvector
corresponding to the largest (positive) eigenvalue for covariance matrices (positive definite). 
Remember that $ ||\mathbf{w}|| = 1 \rightarrow  ||\mathbf{w}|| = ||\mathbf{w}||^2 = \mathbf{w}^T \mathbf{w} = 1$.
The same result can be
computed from Lagrangian constrained optimization. Taking derivatives and equating to zero</p>

<script type="math/tex; mode=display">
\frac{\partial}{\partial \mathbf{w}} \left( \mathbf{w}^T \mathbf{\Sigma} \mathbf{w} - \lambda \left(\mathbf{w}^T \mathbf{w} - 1\right) \right) = 0 \\
\mathbf{\Sigma} \mathbf{w} - \lambda \mathbf{w} = 0
</script>

<p>We arrive at</p>

<script type="math/tex; mode=display">
\mathbf{\Sigma} \mathbf{w}  = \lambda \mathbf{w}
</script>

<p>Which is an eigenvalue problem (and given the nature of $\mathbf{\Sigma}$, a symmetric one, which benefits from efficient
methods to arrive at solutions. The solution to the maximization problem is the eigenvector corresponding to the largest
eigenvalue. Subsequent components, given orthogonality constraints, are computed with subsequent eigenvectors.</p>

<p>Let $\mathbf{U}$ and $\mathbf{D}$ the eigenvector matrix (where the eigenvectors of
$\mathbf{\Sigma}$ are the columns of $\mathbf{U}$) and the eigenvalue matrix with the eigenvalues placed in the diagonal,
I usually choose to add the eigenvalue information to the weight combination matrix $\mathbf{W}$ to account for
component spread, but this is of no practical consequence (you can choose to take $\mathbf{W}=\mathbf{U}$, which amounts
to stacking up the vectors $\mathbf{w}$ in columns).</p>

<script type="math/tex; mode=display">
\mathbf{W} = \mathbf{U} \mathbf{D}^{\frac{1}{2}}
</script>

<p>And the transformation of the data is</p>

<script type="math/tex; mode=display">
\mathbf{Z} = \mathbf{X} \mathbf{W}
</script>

<h3 id="probabilistic-derivation">Probabilistic derivation</h3>

<p>I consider the probabilistic derivation a landmark since a lot of methods have been derived afterwards following
this approach. The seminal paper is <a href="http://research.microsoft.com/pubs/67218/bishop-ppca-jrss.pdf">“Probabilistic Principal Component Analysis”, by M. E. Tipping and C. M. Bishop</a>,
published in The Journal of the Royal Statistical Society, Series B.</p>

<p>The probabilistic approach has several benefits, for example, a natural way of handling incomplete data and obtain
transformations even in the presence of missing attributes. This, however, has been rarely implemented on software
packages.  The <a href="http://rgm3.lab.nig.ac.jp/RGM/R_rdfile?f=pcaMethods/man/ppca.Rd&amp;d=R_BC">Probabilistic PCA R Package</a>
supports incomplete data.</p>

<h3 id="playground-with-sklearn-and-r">Playground with Sklearn and R</h3>

<p>In IPython Notebook, open up a new sheet and import the necessary modules</p>

<p><code>python
import matplotlib
import datetime
from matplotlib.finance import quotes_historical_yahoo
from matplotlib.dates import YearLocator, MonthLocator, DateFormatter
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import numpy as np
</code></p>

<p>Define the date range that we are going to extract prices from</p>

<p><code>python
date1 = datetime.date(2004, 1, 1)
date2 = datetime.date(2013, 1, 6)
</code></p>

<p>Get the historical data</p>

<p><code>python
intc = quotes_historical_yahoo("INTC", date1, date2)
msft = quotes_historical_yahoo("MSFT", date1, date2)
</code></p>

<p>Extract the closing price from the data</p>

<p><code>python
cintc = np.array([q[2] for q in intc])
cmsft = np.array([q[2] for q in msft])
</code></p>

<p>Build a data matrix to be fed to the PCA implementation in a suitable format</p>

<p><code>python
X = np.vstack((cintc,cmsft)).T
</code></p>

<p>Plot both closing prices so that we can visually compare them with the transforms</p>

<p><code>python
plot(X)
</code></p>

<p><img src="/images/pca_intc_msft_stocks.png"></p>

<p>Build the object</p>

<p><code>python
pca = PCA()
</code></p>

<p>Transform the data</p>

<p><code>python
T = pca.fit_transform(X)
</code></p>

<p>Let’s examine the variance ratio explained by each of the components. Notice that the first component explains a lot
of the price movement of both stocks.</p>

<p><code>python
pca.explained_variance_ratio_
</code></p>

<p>with 0.8132162 and 0.1867838 of the variance explained by the components. Now we can print out the PCA transformation</p>

<p><code>python
plot(T)
</code></p>

<p><img src="/images/pca_intc_msft_transform.png"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pythagorean Theorem]]></title>
    <link href="http://analyticbastard.github.io/blog/2014/11/06/pythagorean-theorem/"/>
    <updated>2014-11-06T17:24:02+01:00</updated>
    <id>http://analyticbastard.github.io/blog/2014/11/06/pythagorean-theorem</id>
    <content type="html"><![CDATA[<p>The Pythagorean Theorem is almost mainstream, everybody knows what it is about, yet not so many people know how to prove
it. Here is a proof that I consider to be the easiest one. It’s very analytic and avoids any geometric complications
 such as triangle similarity considerations.</p>

<p>Now, for this proof, you need to know basic geometrical notions such as the area of a square and the area of right
triangles.</p>

<p>Let a (large) square, be cut by a reflecting straight line such as the line builds an inner square and four right 
triangles, such as in the figure below. </p>

<p>Let one of the segments cut by the reflecting line be called $a$, and let the other be called $b$, which makes the
side of the large outer triangle $(a+b)$. In the smaller square,
let the side of the inner square be called $c$. These are the legs and the hypotenuse of the four right triangles.</p>

<p><img class="left" src="/images/pythagorean.png" width="400" height="400" title="‘Pythagorean theorem’ ‘images’" ></p>

<p>Now, we know that the area $L$ of the large is its side squared, if arithmetic holds, 
<script type="math/tex">
L=(a+b)^2=a^2 + b^2 + 2ab
</script> </p>

<p>On the other hand, the large outer square is made up of four right triangles, whose individual area is $\frac{ab}{2}$ and
the inner square, whose area is $c^2$. Thus</p>

<script type="math/tex; mode=display">
L=4 \times \frac{ab}{2} + c^2
</script>

<p>Equating both expressions, we can arrive at the result of the theorem</p>

<script type="math/tex; mode=display">
a^2 + b^2 + 2ab = 4 \frac{ab}{2} + c^2 \\
a^2 + b^2 + 2ab = 2 \times ab + c^2\\
a^2 + b^2 = c^2
</script>

<p>Notice that only axioms about angle reflection and straight lines are needed for this theorem (for example, we need
to establish that a straight line reflects on another straight line with the complementary angle of both straight lines).</p>

<p>I made the image above and it is copylefted. If you want to use the SVG version, feel free to drop me an email.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Gentle Introduction to RKHS and Kernel Methods]]></title>
    <link href="http://analyticbastard.github.io/blog/2014/11/05/a-gentle-introduction-to-rkhs/"/>
    <updated>2014-11-05T00:31:32+01:00</updated>
    <id>http://analyticbastard.github.io/blog/2014/11/05/a-gentle-introduction-to-rkhs</id>
    <content type="html"><![CDATA[<p>A (real or complex) Reproducing kernel Hilbert spaces (RKHS), is a Hilbert space of point-defined functions where the evaluation functional is linear and bounded (equivalently continuous). That the functions are point-wise defined is almost self explanatory, and means that the objects in the space, the functions, are built from defining them at locations within a domain (a compact domain). The way of associating the function to its value at a location is done via the evaluation functional. Think of the evaluation functional <em>at a location</em> as a black box (not so black box in the RKHS) that, given a function as an argument, spits the function value at that location. The fact that the evaluation functionals are linear and bounded roughly means that if an evaluation functional evaluates the sum of two functions (the sum in their vector space), then the results amounts to summing the two evaluations of both functions by the linear functional, i.e.,
<script type="math/tex">
\delta_x(\alpha f + \beta g) = \alpha \delta_x(f) + \beta \delta_x(g)
</script>
for a location $x$, and two real (or complex) numbers $\alpha$ and $\beta$.</p>

<p>This theory was originally developed as a functional analytic abstraction to solve a linear differential equations (as were Hilbert and Banach spaces) of positive (or negative) kind, but made their way to data analysis first in the theory of splines and later became the engine of multiple abstract machines, based on more general reproducing kernels.</p>

<p>Let $\mathbf{X}$ be a compact set or a manifold. The linear property of the evaluation functional implies, by the Riesz representation theorem, that it has a representer within the space, in contrast, for example, to the Dirac’s $\delta$ evaluation functional of the Hilbert space of squared-integrable functions (equivalence classes of functions) $L^2(\mathbf{X})$, which is a generalized function and, thus, does not belong to $L^2(\mathbf{X})$. 
The Riesz representation theorem implies that there is an element <em>within</em> the space of functions that yields the same results when operating it with the rest of the elements of that space than a certain linear functional. Note that this, alone, does not say that all functionals that are evaluation functionals are linear. 
It is only when they are linear that the Riesz theorem applies and they can be associatited to certain elements within the space, and we find ourselves with a RKHS.</p>

<p>A function $f$ belonging to a RKHS $H_k$ can then be evaluated, at any point $\mathbf{x}$ in the set on which the functions in $H_k$ are defined, with the <em>reproducing property</em>
<script type="math/tex">
f(\mathbf{x}) = \delta_{\mathbf{x}} (f) = \langle k_{\mathbf{x}}, f \rangle_{H_k}
</script>
where we call $\delta_x$ to the linear evaluation functional at location $\mathbf{x}$ for $H_k$, belonging to $H_k^{*}$, the algebraic dual of $H_k$, whose representer element in $H_k$ is $k_x$. The notation $k_x = k(\cdot, x) \in H_k$ means that we fix the second argument to $\mathbf{x}$ so that $k$ is a function only on the first argument, and then it belongs to the very space $H_k$ it can reproduce pointwise via inner products. We work with real RKHS henceforth.</p>

<p>This function with two arguments, $k:\mathbf{X} \times \mathbf{X} \rightarrow \mathbb{R}$, is the <em>kernel</em> of the positive linear integral operator 
<script type="math/tex">
T_k : L_X^2(\mathbf{X}) \rightarrow H_k
</script>
such that the bi-linear form
<script type="math/tex">
\langle f, T_k f \rangle_{L^2(\mathbf{X})} = \int_{\mathbf{X}} \int_{\mathbf{X}} k(\mathbf{x}_1,\mathbf{x}_2) f(\mathbf{x}_2) dP_X(\mathbf{x}_2) f(\mathbf{x}_1) dP_X(\mathbf{x}_1)
</script>
is positive, where $P_X$ is a finite Borel measure endowing $\mathbf{X}$ and $f \in L_{P_X}^2(\mathbf{X})$, the Hilbert space of square-integrable functions under measure $P_X$. Such a kernel is called positive-definite.
This of this as the infinite-dimensional equivalent of a matrix, which instead of finite dimensional vectors, operates functions in a linear fashion (the kernel itself does not depend on the function it is operated with under the integral sign). This is the functional-analytic way of solving differential equations, since we can invert the linear differential operator with these kind of integral operators, and this get the solution. </p>

<p>A particularly important class of linear operator kernels are positive-definite kernels, and of particular interest is that of Mercer’s kernels. Given a Mercer’s kernel $k$, it holds that
<script type="math/tex">
k(\mathbf{x}_1,\mathbf{x}_2)=\Phi(\mathbf{x}_1)^T\Phi(\mathbf{x}_2) = \sum_{j=1}^{\infty} \lambda_j \phi_j(\mathbf{x}_1) \phi_j(\mathbf{x}_2)
</script>
where $\lambda_j$ and $\phi_j$ are the eigenvalues and eigenfunctions of the linear integral operator $T_k$, which is compact. This iduces a map $\Phi: \mathbf{X} \rightarrow \ell^2$, the space of square-summable sequences. This map $\Phi = (\sqrt{\lambda_1} \phi_1, \sqrt{\lambda_2} \phi_2, \ldots)^T$ is the so-called <em>kernel embedding</em> and allows the practitioner to map the data to a definite dimension (possibly infinite), where the learning task is likely to become linear. This map, however, needs not be computed explicitly in Kernel Learning methods and is defined, as seen above, by the kernel we are using and, in practice, we are restricted to a finite number of training points. This map can be written as a vector $\mathbf{z} \in \mathbb{R}^D$ for each datum, with a dimension $D \leq N$. The collection for $N$ data can be written as a matrix $\mathbf{Z} \in \mathbb{R}^{N \times D}$ of vectors $\mathbf{z}$ vertically stacked, and the finite version of the Mercer kernel expansion, yields the finite-dimensional embedding $\mathbf{Z}$ and the so-called <em>kernel trick</em>
<script type="math/tex">
\mathbf{K} = \mathbf{Z} \mathbf{Z}^T
</script>
where $\mathbf{K} \in \mathbb{R}^{N \times N}$ is the Gram matrix of kernel evaluations on the dataset <script type="math/tex">\{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N\}</script> built so that each entry of the matrix correspond to the kernel evaluation at two points, $\mathbf{K}_{i,j} = k(\mathbf{x}_i, \mathbf{x}_j)$. This embedding constituted the breakthrough in Machine Learning in the late 90s and early new century. Notice that this feature space is <strong>not</strong> the RKHS, since it is a subspace of $\ell^2$, their elements being sequences, whereas the RKHS is a subspace if $L^2$, its elements being functions.</p>

<p>Let $f,g \in H_k$ be expressed as a linear combination of the eigenfunctions of $T_k$, <script type="math/tex">f = \sum_{j=1}^{\infty} \alpha_j \phi_j</script> and <script type="math/tex">g = \sum_{j=1}^{\infty} \beta_j \phi_j</script>. The inner product in $H_k$ is defined as
<script type="math/tex">
\langle f,  g \rangle_{H_k} = \sum_{j=1}^{\infty} \frac{\alpha_j \beta_j}{\lambda_j}
</script>
Which is equal to the equation above, i.e., 
<script type="math/tex">\langle f,  g \rangle_{H_k} = \langle f, T_k g \rangle_{L^2(\mathbf{X})}
</script>.
This induces a norm
<script type="math/tex">
\|f\|_{H_k}^2 = \langle f,  f \rangle_{H_k} =  \|P f\|_{L^2(\mathbf{X})}^2
</script>
Where $P$ is a pseudo-differential operator and <script type="math/tex">P^{*}</script> is its adjoint such that <script type="math/tex">T_k = P^{*} P</script>.</p>

<p>The learning task in kernel methods consist on computing a function that can approximate certain pre-defined data with certain desired properties, namely, that it is “simple” enough, simplicity measured by the norm in the space, the smaller norm, the better, as this requirement tries to overcome the overfitting and noise presence (the less “wiggly” the function, the less it responds to noise). So given a kernel function $k$ and a finite data sample of size $N$, the Representer theorem ensures that the subspace of functions spanned by finite linear combinations of the form $f=\sum_j^{n}{\alpha_j k(\cdot, \mathbf{x}_j)}$ is the solution to a regularization problem of the form
<script type="math/tex">
\min_{f \in H_k}  \sum_{j=1}^{n} (y_i - f(\mathbf{x}_i))^2 + \lambda \|f\|_{H_k}^2
</script>
where $\lambda$ is the regularization parameter, and where we see that both the approximation results on our existing training data and the complexity of the function are accounted for. This kind of quadratic functionals appear in multiple kernel methods, including support vector machines (SVM). These quadratic functionals are the ones optimized by any quadratic optimization method to arrive at an acceptable solution.</p>

<p>Despite the proven power of kernel methods, they have a main drawback, which is that they scale with the square of the number of data $N$. Managing matrices larger than $N&gt;10000$ is unmanageable for most computers, and the $N^2$ scaling renders supercomputers unable to handle these matrices. To partially overcome this difficulty, several methods have been developed. Firstly, the Nyström method can be used to approximate any kernel Gram matrix simply using the fact that for an integral linear operator as in the above equation, the eigenequation
<script type="math/tex">
\int_{\mathbf{X}} k(\mathbf{x}_1,\mathbf{x}_2) \phi_i(\mathbf{x}_2) dP_X(\mathbf{x}_2) = \lambda_i \phi_i(\mathbf{x}_1)
</script>
holds, so that uniformly sampling $\mathbf{x}$ from $P_X$, we can make the approximation of the kernel matrix
<script type="math/tex">
\mathbf{K} \mathbf{u} = \lambda \mathbf{u}
</script>
with the restricted subsample of size $q$, where $\mathbf{u}$ is an eigenvector approximation. Then, one can obtain an approximation to the first eigenvectors of the matrix, which have the following expressions
<script type="math/tex">
\phi_i(\mathbf{x}) \approx \sqrt{q} \mathbf{u}_i^{(q)} \quad\quad \lambda_i \approx \frac{\lambda_i^{(q)}}{q}
</script></p>

<p>Another well-known approximation is the random features technique, this time only for translation invariant kernels (i.e., those which can be written <script type="math/tex">k(\mathbf{x}_1,\mathbf{x}_2)=k(\mathbf{x}_1-\mathbf{x}_2)</script>, abusing notation and writing $k$ for both functions) was developed by Rahimi and Rech. Bochner’s theorem is a result from classical harmonic analysis and applies to the mentioned kernel types. A continuous function <script type="math/tex">k \in L^1(\mathbb{R}^N)</script> is positive definite if and only if it is the Fourier transform of a non-negative measure $\Lambda$.
<script type="math/tex">
k(\mathbf{x}_1-\mathbf{x}_2)=\int_{\mathbb{R}^N}{e^{i\mathbf{\omega}^T (\mathbf{x}_1-\mathbf{x}_2)}d \Lambda(\mathbf{\omega})}
</script>
The method, then, consist on sampling (multivariate) frequencies $\mathbf{\omega}$ from the probability distribution $\Lambda$ related to the kernel $k$ and build feature vectors from the Fourier complex exponentials $e^{-i\mathbf{\omega}^T \mathbf{x}}$, pairs of sines and cosines at frequency $\mathbf{\omega}$, or using only a cosine at frequency $\mathbf{\omega}$ with phase $b$ sampled from a uniform distribution between zero and $2\pi$. The kernel approximation at a point is, then, the product of all the features at that point. Comparison between both methods have been made and it has been found that Nyström method has a number of advantages, such as approximating speed, stemming mainly from the dependence on the distribution of the data $P_X$, whereas random features proceed independently of $P_X$. This idea has also been used in Gaussian Processes with success.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Improve the Performance of an SVM With Differential Geometry]]></title>
    <link href="http://analyticbastard.github.io/blog/2014/11/03/improve-the-performance-of-an-svm-with-differential-geometry/"/>
    <updated>2014-11-03T03:19:37+01:00</updated>
    <id>http://analyticbastard.github.io/blog/2014/11/03/improve-the-performance-of-an-svm-with-differential-geometry</id>
    <content type="html"><![CDATA[<p>Though I am not very wise concerning differential geometry (others aren’t either, but they claim to be researching on the field), I find it amusing to read a little bit of it when it is used along with kernel methods, and especially when you can improve the behavior of a SVM with it.</p>

<p><a href="http://www.dcs.warwick.ac.uk/~feng/papers/Scaling%20the%20Kernel%20Function.pdf">Amari and Wu</a> are responsible for the following method: The idea is that, in order to increase class separability, we need to enlarge the spatial resolution around the boundary in the feature space. Take, for instance, the Riemannian distance along the manifold</p>

<script type="math/tex; mode=display">
ds^2 = \sum_{i,j} g_{i,j} dx_i dx_j
</script>

<p>We need it to be large along the border of $f(\mathbf{x})=0$ and small between points of the same class. In practice, the boundary is not known, so we use the points the we know are closest to the boundary: the support vectors. A conformal transformation does the job</p>

<script type="math/tex; mode=display">
\tilde{g}_{i,j}(\mathbf{x}) = \Omega (\mathbf{x}) g_{i,j} (\mathbf{x})
</script>

<p>This is very difficult to realize practically, so we consider a quasi-conformal transformation to induce the a similar map by directly modifying</p>

<script type="math/tex; mode=display">
\tilde{K}(\mathbf{x_1},\mathbf{x_2}) = c(\mathbf{x_1}) c(\mathbf{x_2}) K(\mathbf{x_1},\mathbf{x_2})
</script>

<p>where $c(\mathbf{x})$ is a positive function, that can be built from the data as</p>

<script type="math/tex; mode=display">
c(\mathbf{x}) = \sum_{i \in SV} h_i e^{\frac{\| \mathbf{x} - \mathbf{x}\|^2}{2\tau^2}}
</script>

<p>where $h_i$ is a parameter of the $i$-th support vector.</p>

<p>Thus, if you first train a SVM with a standard kernel, and then you compute $c(x)$ and make a new kernel with the previous expressions, your SVM will behave better.</p>

<p>The authors report higher classification accuracy and less support vectors than with standard kernels.</p>

<p><sub>This post is part of my old blog. See the original <a href="http://machinomics.blogspot.co.uk/2012/09/improve-performance-of-your-svm.html">here</a></sub></p>
]]></content>
  </entry>
  
</feed>
