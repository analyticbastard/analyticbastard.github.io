<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Statistics | The Analytic Bastard]]></title>
  <link href="http://analyticbastard.github.io/blog/categories/statistics/atom.xml" rel="self"/>
  <link href="http://analyticbastard.github.io/"/>
  <updated>2015-03-30T01:33:06+02:00</updated>
  <id>http://analyticbastard.github.io/</id>
  <author>
    <name><![CDATA[Analytic Bastard]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[The Monty Hall Problem (My Explanation)]]></title>
    <link href="http://analyticbastard.github.io/blog/2015/03/08/the-monty-hall-problem-my-explanation/"/>
    <updated>2015-03-08T00:17:17+01:00</updated>
    <id>http://analyticbastard.github.io/blog/2015/03/08/the-monty-hall-problem-my-explanation</id>
    <content type="html"><![CDATA[<p>If I tell you: There are 1K bitcoins 1 wallet that will be yours if you guess which wallet out of three is the right one, the rest containing an amount of zero bitcoins, and ask you to point out an initial selection, then showed you that, effectively, one of the remaining wallets contains zero bitcoins… and finally giving you the opportunity to change wallet. Would you change? The awnser is yes.</p>

<p>This is so because there is new evidence now that supports a higher probability that the remaining unseen wallet is the right choice, whereas there is none about your current choice. The fact that you selected wallet 1, and given that choice, I showed you wallet 2, that leaves wallet 3 with a posterior probability of 2/3. This does not happen for our current wallet 1, since choosing 1 influenced my decision to show you 2. More precisely: You chose wrongly with probability 2/3. With that probability, I show you the only possible door that I can, leaving the 2/3 for the remaining unseen and unchosen door. On the contrary, you choose well with 1/3 probability, but then I can choose among 2 doors to show you, each with a probability of 1/2. This is how we include my decision (or necessity) to show you 2 into the math (this is the best explanation you are gonna get from all over the internet):
Let’s call R “right choice” V “visible incorrect wallet” and S “your choice”. We need to compute <script type="math/tex">P(R=3|V=2,S=1)</script>, the probability of 3 being the right wallet, after you selected 1 and I showed you that 2 was not right (remember that all priors are 1/3).
<script type="math/tex">P(R=3|V=2,S=1)=\frac{P(V=2,S=1|R=3)P(R=3)}{P(V=2,S=1|R=3)P(R=3)+P(V=2,S=1|R=1)P(R=1)}\\=\frac{1\times 1/3}{1\times 1/3 + 1/2 \times 1/3}=2/3</script><script type="math/tex">P(V=2,S=1|R=3)=1</script> is the probability that, given R=3, then I was forced to show you the incorrect wallet remaining (you already chose one incorrect wallet). <script type="math/tex">P(V=2,S=1|R=1)=1/2</script> because there are two possible incorrect wallets (since you selected the correct one) that I can choose from to show you.</p>

<p>Let’s compute the same posterior for the case I decide not to change wallet:
<script type="math/tex">P(R=1|V=2,S=1)=\frac{P(V=2,S=1|R=1)P(R=1)}{P(V=2,S=1|R=3)P(R=3)+P(V=2,S=1|R=1)P(R=1)}\\=\frac{1/2\times 1/3}{1\times 1/3 + 1/2 \times 1/3}=1/3</script>.</p>

<p>Therefore if you change you have more chances of winning the 1000 bitcoins.</p>

<p>Needless to say, this works for every possible combination of <script type="math/tex">R</script>, <script type="math/tex">S</script> and <script type="math/tex">V</script>. 
This happens, as I mentioned, because of the way I was influenced (forced) to show you the incorrect remaining wallets. To see it intuitively, imagine 100 wallets, and that you chose one amongst them, and I am forced to show you 98 incorrect wallets, leaving your choice and another one. Is it more likely that this particular wallet is the correct one (that your choice forced me to leave it) or that you chose wisely amongst 100 wallets? If you choose 99 incorrect wallets, the set that I show you is the same, except for the chosen incorrect wallets each time, and will never contain the particular correct wallet.</p>

<p>There is a cool <a href="https://play.google.com/store/apps/details?id=us.steveo.montyhall">Android app</a> in case you want to check how the law of large numbers works for this problem.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Image Pan-sharpening With PCA]]></title>
    <link href="http://analyticbastard.github.io/blog/2015/02/20/image-pan-sharpening-with-pca/"/>
    <updated>2015-02-20T11:27:48+01:00</updated>
    <id>http://analyticbastard.github.io/blog/2015/02/20/image-pan-sharpening-with-pca</id>
    <content type="html"><![CDATA[<p>There is a collection of methods that are denominated image pan-sharpening, since they use
information available in multiple bands to create an image output that enjoys properties
from all sources, while minimizing drawbacks. The sources combined will contain several
frequency bands, maybe hundreds or thousands. In this case, we are interested in combining
a source with a high resolution but in gray scale, and a RGB image.</p>

<p>This is especially important in satellite imagery, since satellites often incorporate
a multispectral camera with limited resolution and a high-resolution, low-band/grayscale
camera among their equipment.</p>

<p>To play with this, we will use Matlab (since I’ve worked a little bit with its image
functions and array transformations, and this job will be quick)</p>

<h2 id="data-preparation">Data preparation</h2>

<p>We first load original image (included in Matlab)</p>

<p><code>matlab
peppers = imread('peppers.png');
</code></p>

<p><img class="center" src="/images/peppers.png" title="‘Original image’ ‘images’" ></p>

<p>Now we simulate uniband (grayscale) and multiband (downscaled) imagery</p>

<p><code>matlab
multiband = imresize(peppers, 0.25);
uniband = rgb2gray(peppers);
</code></p>

<p>The multiband image looks like this</p>

<p><img class="center" src="/images/multiband.png" title="‘Multiband image’ ‘images’" ></p>

<p>And the high resolution (low band) image looks like this</p>

<p><img class="center" src="/images/uniband.png" title="‘High resolution image’ ‘images’" ></p>

<p>In the first case, we resize the image to a quarter of its size <strong><em>in both dimensions</em></strong>.
This means we loose 15/16 of the information! In the second case we compute a grayscale
image, loosing complementary information.</p>

<p>These two images are the ones we are interested in combining.</p>

<h2 id="image-pan-sharp-with-pca">Image pan-sharp with PCA</h2>

<p>We still need to prepare the data one more time. 
We need to upscale multiband image (again, color image and same size than original but with
the information from the downscaled version) so that we can combine it with the hight
resolution image (adjust its dimensions)</p>

<p><code>matlab
interpolated=imresize(multiband,[size(uniband,1) size(uniband,2)]);
</code></p>

<p><img class="center" src="/images/interpolated.png" title="‘Interpolated image from the low resolution, hight band image’ ‘images’" ></p>

<p>The missing information from this image with respect to the original is noticeable
to the eye in form of blur.</p>

<p>We will now perform PCA on the images. To do that we need to convert the images to the
relevant variables we want to consider in the analysis. In this case, we want to extract
the bands of the images, so that we linearize the images, thus turning matrices into
unidimensional vectors. In case of the grayscale image, this is easily done by just
using the <code>(:)</code> operator. Bit in case of the RGB image, we will need to
reshape into a three-dimensional multivariate X = (R, G, B)</p>

<p><code>matlab
X=double(reshape(interpolated,numel(interpolated)/3,3));
</code></p>

<p>where we have also converted the data type to double so that we can use Matlab functions.</p>

<p>Now we can compute the components and the projections onto the PCA space</p>

<p><code>matlab
[C,Y]=pca(X);
</code></p>

<p>where <code>C</code> is the matrix of loadings and <code>Y</code> is the projections of the data onto
the PCA space.</p>

<p>At this point, out of curiosity, we can compute correlations. Notice that the first
projection will correlate a lot with the univariate, since both capture mean luminosity
levels, which is an indicator of which component can be substituted by the higher
resolution grayscale image.</p>

<p><code>matlab
corrcoef(Y(:,1),double(uniband(:)))
corrcoef(Y(:,2),double(uniband(:)))
corrcoef(Y(:,3),double(uniband(:)))
</code></p>

<p>The first one yields around <code>0.98</code>. Substitute the component now</p>

<p><code>matlab
Z=Y;
Z(:,1)=(double(uniband(:)-min(uniband(:)))./double(max(uniband(:))-min(uniband(:))))*(max(Y(:,1))-min(Y(:,1)))+min(Y(:,1));
</code></p>

<p>We are adapting the maximum and minimum luminosity of the grayscale image to match
that of the first component.</p>

<p>We now project the new components back to the original RGB space. The matrix <code>C</code> is not
orthonormal, so that we don’t get a properly scaled representation, so we do it manually
(we could also make the matrix <code>C</code> orthonormal, though the effect of changing the first
component would have impacted the final luminosity levels and thus we still would have needed
to rescale the colors)</p>

<p><code>matlab
G=Z*C';
I = (G - min(G(:)))/(max(G(:))-min(G(:)));
</code></p>

<p>Lastly we reshape from (R, G, B) multivariate to an RGB image</p>

<p><code>matlab
merged = reshape(I, size(interpolated));
</code></p>

<p>The result shows that, effectively, we enjoy the best part of the two worlds</p>

<p><img class="center" src="/images/merged.png" title="‘Merged image’ ‘images’" ></p>

<p>As mentioned, we could play with the luminiscence levels to attain a color more loyal
to the original.</p>

<p>The reason why this works is that the first PCA component takes most of the information,
thus creating a sort of best fit for all colors in the RGB spectrum. This is a kind of
luminiscence, which is what the grayscape image is and what we substitute it by.</p>

<p>Donwload full Matlab program form <a href="https://github.com/analyticbastard/pansharpening-pca">Github</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Principal Component Analysis]]></title>
    <link href="http://analyticbastard.github.io/blog/2015/01/01/principal-component-analysis/"/>
    <updated>2015-01-01T01:22:01+01:00</updated>
    <id>http://analyticbastard.github.io/blog/2015/01/01/principal-component-analysis</id>
    <content type="html"><![CDATA[<p>Principal Component Analysis is a classical statistical technique that aims at finding a transformation of the
input or measured variables so that the transformed variables offer a view of the data that maximizes the presented
“information” about the dataset. This allows for dimensionality reduction, since we can select a number of dimensions
which provide most of the “information” and be sure that each of the rest of the discarded dimensions contain less
information than each of the dimensions we have retained.</p>

<p>We start by requesting something as simple as our solution to be a constrained linear combination (so that the
coefficient vector is of norm one) so that it is of maximum square norm.</p>

<script type="math/tex; mode=display">
\max_{\|\mathbf{w}\|=1}  || \mathbf{X} \mathbf{w} ||^2 \\
\mbox{s.t. }  \mathbf{w}^T \mathbf{w_i} = 0
</script>

<p>Maximizing the square norm explains why the first component of non-centered data contains some kind of a data average
for example, the first <a href="http://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html">eigenface</a> .</p>

<p><!-- Image -->
<a id="img-13" class="imgModal floatRight" href="#imgModal-13" data-toggle="modal">
  <img src="/images/plot_faces_decomposition_002.png" width="300" height="226" title="Click for larger view." />
</a>
<div style="float: none;"></div>

<!-- Modal -->
<div class="modal fade" id="imgModal-13" tabindex="-1" role="dialog" aria-labelledby="imgModal-13Label" aria-hidden="true">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
      </div>
      <div class="modal-body">
        <img src="/images/plot_faces_decomposition_002.png" width="600" height="451" />
      </div>
      <div class="modal-footer">
        <button type="button" class="btn btn-primary" data-dismiss="modal">Close</button>
      </div>
    </div><!-- /.modal-content -->
  </div><!-- /.modal-dialog -->
</div><!-- /.modal --></p>

<p>where, in the case of the first principal component, there is no $i$ less than one and the constraint does not apply.</p>

<p>Since the scalar product induces a norm, $ \mathbf{w}^T \mathbf{w}$ is the norm of $\mathbf{w} $ and the above becomes</p>

<p>Let $\mathbf{X}$ be the data matrix, let $\mathbf{\Sigma} = \mathbf{X}^T \mathbf{X}$ be the covariance matrix.</p>

<script type="math/tex; mode=display">
\max_{\|\mathbf{w}\|=1}  \mathbf{w}^T \mathbf{\Sigma} \mathbf{w} \\
\mbox{s.t. }  \mathbf{w}^T \mathbf{w_i} = 0
</script>

<p>For the first principal component, this is an quadratical optimization problem constrained to unitary norm of the
solution. This can be written as the optimization of a Rayleigh quotient.</p>

<script type="math/tex; mode=display">
\max_{\mathbf{w}} \frac{\mathbf{w}^T \mathbf{\Sigma} \mathbf{w}}{||\mathbf{w}||^2} \\
\max_{\mathbf{w}} \frac{\mathbf{w}^T \mathbf{\Sigma} \mathbf{w}}{\mathbf{w}^T \mathbf{w}} \\
</script>

<p>This is a Rayleigh quotient and it is well known from Matrix Analysis that the solution is in terms of the eigenvector
corresponding to the largest (positive) eigenvalue for covariance matrices (positive definite). 
Remember that $ ||\mathbf{w}|| = 1 \rightarrow  ||\mathbf{w}|| = ||\mathbf{w}||^2 = \mathbf{w}^T \mathbf{w} = 1$.
The same result can be
computed from Lagrangian constrained optimization. Taking derivatives and equating to zero</p>

<script type="math/tex; mode=display">
\frac{\partial}{\partial \mathbf{w}} \left( \mathbf{w}^T \mathbf{\Sigma} \mathbf{w} - \lambda \left(\mathbf{w}^T \mathbf{w} - 1\right) \right) = 0 \\
\mathbf{\Sigma} \mathbf{w} - \lambda \mathbf{w} = 0
</script>

<p>We arrive at</p>

<script type="math/tex; mode=display">
\mathbf{\Sigma} \mathbf{w}  = \lambda \mathbf{w}
</script>

<p>Which is an eigenvalue problem (and given the nature of $\mathbf{\Sigma}$, a symmetric one, which benefits from efficient
methods to arrive at solutions. The solution to the maximization problem is the eigenvector corresponding to the largest
eigenvalue. Subsequent components, given orthogonality constraints, are computed with subsequent eigenvectors.</p>

<p>Let $\mathbf{U}$ and $\mathbf{D}$ the eigenvector matrix (where the eigenvectors of
$\mathbf{\Sigma}$ are the columns of $\mathbf{U}$) and the eigenvalue matrix with the eigenvalues placed in the diagonal,
I usually choose to add the eigenvalue information to the weight combination matrix $\mathbf{W}$ to account for
component spread, but this is of no practical consequence (you can choose to take $\mathbf{W}=\mathbf{U}$, which amounts
to stacking up the vectors $\mathbf{w}$ in columns).</p>

<script type="math/tex; mode=display">
\mathbf{W} = \mathbf{U} \mathbf{D}^{\frac{1}{2}}
</script>

<p>And the transformation of the data is</p>

<script type="math/tex; mode=display">
\mathbf{Z} = \mathbf{X} \mathbf{W}
</script>

<h3 id="probabilistic-derivation">Probabilistic derivation</h3>

<p>I consider the probabilistic derivation a landmark since a lot of methods have been derived afterwards following
this approach. The seminal paper is <a href="http://research.microsoft.com/pubs/67218/bishop-ppca-jrss.pdf">“Probabilistic Principal Component Analysis”, by M. E. Tipping and C. M. Bishop</a>,
published in The Journal of the Royal Statistical Society, Series B.</p>

<p>The probabilistic approach has several benefits, for example, a natural way of handling incomplete data and obtain
transformations even in the presence of missing attributes. This, however, has been rarely implemented on software
packages.  The <a href="http://rgm3.lab.nig.ac.jp/RGM/R_rdfile?f=pcaMethods/man/ppca.Rd&amp;d=R_BC">Probabilistic PCA R Package</a>
supports incomplete data.</p>

<h3 id="playground-with-sklearn-and-r">Playground with Sklearn and R</h3>

<p>In IPython Notebook, open up a new sheet and import the necessary modules</p>

<p><code>python
import matplotlib
import datetime
from matplotlib.finance import quotes_historical_yahoo
from matplotlib.dates import YearLocator, MonthLocator, DateFormatter
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import numpy as np
</code></p>

<p>Define the date range that we are going to extract prices from</p>

<p><code>python
date1 = datetime.date(2004, 1, 1)
date2 = datetime.date(2013, 1, 6)
</code></p>

<p>Get the historical data</p>

<p><code>python
intc = quotes_historical_yahoo("INTC", date1, date2)
msft = quotes_historical_yahoo("MSFT", date1, date2)
</code></p>

<p>Extract the closing price from the data</p>

<p><code>python
cintc = np.array([q[2] for q in intc])
cmsft = np.array([q[2] for q in msft])
</code></p>

<p>Build a data matrix to be fed to the PCA implementation in a suitable format</p>

<p><code>python
X = np.vstack((cintc,cmsft)).T
</code></p>

<p>Plot both closing prices so that we can visually compare them with the transforms</p>

<p><code>python
plot(X)
</code></p>

<p><img src="/images/pca_intc_msft_stocks.png"></p>

<p>Build the object</p>

<p><code>python
pca = PCA()
</code></p>

<p>Transform the data</p>

<p><code>python
T = pca.fit_transform(X)
</code></p>

<p>Let’s examine the variance ratio explained by each of the components. Notice that the first component explains a lot
of the price movement of both stocks.</p>

<p><code>python
pca.explained_variance_ratio_
</code></p>

<p>with 0.8132162 and 0.1867838 of the variance explained by the components. Now we can print out the PCA transformation</p>

<p><code>python
plot(T)
</code></p>

<p><img src="/images/pca_intc_msft_transform.png"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Equality of Means Statistical Test Made Easy]]></title>
    <link href="http://analyticbastard.github.io/blog/2014/12/30/equality-of-means-statistical-test/"/>
    <updated>2014-12-30T01:22:01+01:00</updated>
    <id>http://analyticbastard.github.io/blog/2014/12/30/equality-of-means-statistical-test</id>
    <content type="html"><![CDATA[<p>I never realized how difficult it was to understand a simple statistical concept such as the test for equality of means
until I had to explain it to my little sister. And I found out that this difficulty stemmed from poor explanations of
my professors and even worse from the textbooks I used to study it. I always ended up using the cookbook recipe,
stating the null hypothesis, performing the computations and stating the results in the correct statistical language.
I never had to explain it in my academic life so I never confronted this particular piece of knowledge and the root of
its difficulty.</p>

<p>My little sister could not understand the meaning of (talking about box plots):</p>

<p><blockquote><p>If the notches overlap, we reject the null hypothesis (that the means are different) at the 95% level.</p></blockquote></p>

<p>Even more, the textbook was talking about medians, which made the whole thing a lot more convoluted.</p>

<p>I was shocked, and certainly ashamed, I could not even begin to articulate any word. I was naked, I didn’t know anything
about it. “OK”, I thought, “let’s start from the beginning. What do you <em>really</em> want to do? What you want to do is
test if the two medians of both populations are equal or not, in a statistical sense”. I located a perniciously hidden
important constraint: The variables are normally distributed. “OK, that’s it”. Since their distribution is normal, 
which is symmetrically distributed around the mean, the median turns out to be the mean (and also the mode). The test
is reduced to an equality of means. Now, what would I do with two averages of two samples taken from two normally
distributed populations? For that, one must recall the difference between the average and the mathematical expectation
of a probability distribution (the first moment). The average is an aggregate of the different values of a sample.</p>

<script type="math/tex; mode=display">
\bar{x} = \frac{1}{N} \sum_{i=1}^N x_i
</script>

<p>Each of the values come from a distribution, which means that each of them is a random variable that acquires a
particular value. This means that the aggregate is also normally distributed.</p>

<script type="math/tex; mode=display">
\bar{X} = \frac{1}{N} \sum_{i=1}^N X_i \sim N(\mu, \sigma)
</script>

<p>This $\mu$ is the mathematical expectation. Recall that this is a linear operator in the functional analytic sense
of the word <em>operator</em>, that takes a function (probability density) and spits a number in the field the distribution
domain is a subset of (we assume it is the real numbers).</p>

<p>If the random variable is discrete, the operator $E[\cdot] : \ell \rightarrow \mathbb{R}$ is defined as</p>

<script type="math/tex; mode=display">
E[X] = \sum_{i \in \mathbb{I}} x_i p_i
</script>

<p>If it is otherwise continuous, $E[\cdot] : \mathbb{F} \rightarrow \mathbb{R}$ is:</p>

<script type="math/tex; mode=display">
E[X] = \int_{\mathbb{X}} x p(x) dx
</script>

<p>Where $\ell$ (for the lack of a better symbol) is a set of number sequences and $\mathbb{F}$ is a space of functions
whose analysis lies beyond the purpose of this post (just treat them as sets that contain respectively discrete and
continuous density functions). And $p$ (and $p_i$) is the probability density function or the density frequencies.</p>

<p>Then, we know by the law of large numbers that the average converges to the mathematical expectation as the number
of data $N$ tends to infinity.</p>

<script type="math/tex; mode=display">
\bar{x} \rightarrow E[X] = \mu
</script>

<p>Now, with two random variables $X$ and $Y$, under the null hypothesis, their mathematical expectation, or population
means, are the same:</p>

<script type="math/tex; mode=display">
H_0 : \mu_X = \mu_Y
</script>

<p>So that $\mu_0 - \mu_1 = 0$. This is, the difference must be zero. Now, this is a technicality: The difference is a new
random variable $F=X-Y$ with $E[X-Y]=E[X]-E[Y]=0$ because the operator is linear and the variables
come from the same distribution. <em>This difference is what we use in the test</em>. If we knew more about how it is
distributed, <em>we could query it for possible values to statistically validate the null hypothesis</em> $H_0$. </p>

<p>It turns out that this new difference random variable is also normally distributed and centered
around zero. We can then compute te standard deviation and fix the shape of the distribution, and place a confidence
interval so that the value resulting of the difference of means falls within the 95% of probability density around zero.
This means that if the sample difference is larger that the percentile enclosing this 95% of probability, we are not
<em>accepting</em> the null hypothesis (remember, that $\mu_X = \mu_Y$). Note that with probability 0.05, we are going to get a
sample difference larger than that percentile. For this reason, one does not say <em>reject</em>: There is a chance that the
population means are truly different, bit there is also a small (0.05 probability) chance that, being the population
means the same, we got difference of averages larger than the boundary percentile).</p>

<p>Of course, <em>the true statistical test is divided by a kind of standard deviation made up of an aggregate of the variances
of both samples, which makes the statistic be distributed as a t distribution. Otherwise the test would have no power.</em></p>

<p>To sum up, the statistical test for equality of means is:</p>

<ul>
  <li>Two samples from two populations.</li>
  <li>I want to know whether the means are equal.</li>
  <li>Under the hypothesis that they are equal (constrained to normality of populations and independence of samples):
    <ul>
      <li>The difference of means is zero</li>
      <li>Therefore, the difference between sample averages must be small, and it is a random variable itself that must be
normally distributed around zero.</li>
      <li>Once we compute the standard deviation, and hence the shape of this normal distribution of the difference, we
can select the interval to achieve the desired probability to find the value difference of means in the range,
say, 95%. If it falls outside, then we do not accept the null hypothesis (that the difference is zero). We do
this because it should be extremely rare for us to find two samples whose average difference falls ourside the
range (5%) if the null hypothesis is true.</li>
      <li>If it falls inside the 95% range, then there is no statistical evidence against the null hypothesis.</li>
    </ul>
  </li>
</ul>
]]></content>
  </entry>
  
</feed>
